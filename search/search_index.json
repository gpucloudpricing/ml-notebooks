{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Deep Learning Tutorials and Notebooks","text":"<p>This repository is for my students.</p>"},{"location":"#tutorials","title":"Tutorials","text":""},{"location":"#deep-learning-general","title":"Deep Learning General","text":"<ul> <li>Access Snellius GPUs</li> <li>Deep Learning Project Setup</li> <li>Pytorch Lightning</li> <li>Practical Information about Data</li> <li>Shared Jupyter Notebook in SRC (SURF Research Cloud)</li> <li>Gradio App in SRC (SURF Research Cloud)</li> <li>Hugging Face Guide</li> <li>Set Up a Custom Python Function within PostgreSQL</li> </ul>"},{"location":"#training-and-inference","title":"Training and Inference","text":"<ul> <li>Training and Inference with Limited Resources</li> <li>Improve Deep Learning Training Results</li> <li>DDP(Distributed Data Parallel) in PyTorch</li> </ul>"},{"location":"#courses-and-literature","title":"Courses and Literature","text":"<ul> <li>Courses in Deep Learning and Computer Vision</li> <li>Quick Overview of Object Detection</li> </ul>"},{"location":"#blogpost","title":"Blogpost","text":"<ul> <li>Example Tasks for Large Language, Multimodal, and Vision Models</li> <li>Handling Variable Sequence Lengths in Transformer Models</li> <li>Companies</li> </ul>"},{"location":"#general","title":"General","text":"<ul> <li>Services</li> <li>Git Basics: A Simple Manual</li> <li>Python Basics</li> <li>Docker Basics</li> <li>Tmux Basics</li> <li>Linux Basics</li> </ul>"},{"location":"#examples","title":"Examples","text":"<ul> <li>Scheduler</li> </ul>"},{"location":"#notebook-examples","title":"\ud83d\ude80 Notebook Examples","text":"notebook open in colab Colab basics Training basics Classification Example Object detection with yolov8 and roboflow"},{"location":"companies/","title":"Companies","text":"<p>The methodology employed categorizes companies based on their primary focus or offerings, detailing each company's capabilities alongside its name.</p> <p>Key to Keywords: - Pre: Pretraining capabilities or services - FT: Finetuning services - Custom: Support for building custom models - API: Offers an API to integrate ML functionality - Dev: Development tools or environments - Dep: Deployment tools or platforms - Model: Develops or provides their own ML models - GPU: GPU-based compute resource provider - Storage: Data storage provider - Data: Data platform or data indexing provider</p>"},{"location":"companies/#model-creators-foundation-model-providers","title":"Model Creators / Foundation Model Providers","text":"<ul> <li>OpenAI: Model, Pre, FT, API  </li> <li>Anthropic: Model, Pre, FT  </li> <li>Google DeepMind: Model, Pre, FT, API  </li> <li>xAI: Model, Pre, FT, API  </li> <li>Meta AI: Model, Pre, FT, API  </li> <li>Mistral AI: Model, Pre, FT, API  </li> <li>Stability AI: Model, Pre, FT  </li> <li>Nous Research: Model, Pre, FT  </li> <li>Cohere: Model, Pre, FT, API  </li> <li>Allen Institute for AI (Ai2): Model, Pre, FT  </li> <li>EleutherAI: Model (Open-Source Models)  </li> <li>AssemblyAI: Model, API  </li> <li>Deepgram: Model, API  </li> <li>ElevenLabs: Model, API  </li> <li>Perplexity AI: Model, API</li> </ul>"},{"location":"companies/#apiinference-providers","title":"API/Inference Providers","text":"<ul> <li>OpenRouter: API</li> <li>Replicate: API, GPU, Custom  </li> <li>Groq.AI: API  </li> <li>Together AI: API, FT, Custom, GPU  </li> <li>Novita.UI: API  </li> <li>Lepton.ai: API  </li> <li>Hyperbolic.xyz: API, GPU  </li> <li>Fireworks AI: API  </li> <li>Baseten: API  </li> <li>Deepinfra: API  </li> <li>Octo AI: API</li> </ul>"},{"location":"companies/#development-tools-code-generation-and-productivity","title":"Development Tools, Code Generation, and Productivity","text":"<ul> <li>Cursor: Dev </li> <li>Replit AI: Dev  </li> <li>Amazon CodeWhisperer: Dev</li> <li>Anthropic Computer Use</li> <li>GitHub Copilot: Dev  </li> <li>Codeium: Dev </li> <li>Devin: Dev  </li> <li>Cognition Labs: Dev  </li> <li>OpenHands: Dev  </li> </ul>"},{"location":"companies/#deployment-tools-and-platforms","title":"Deployment Tools and Platforms","text":"<ul> <li>Vercel: Dev, Dep  </li> </ul>"},{"location":"companies/#libraries-and-frameworks","title":"Libraries and Frameworks","text":"<ul> <li>DSPy: Dev (Provides a programming model for developing and optimizing language model pipelines)  </li> <li>Oxolotl (Daniel Han): Fast Training (Pre/FT related)  </li> <li>Torchrun: Dev  </li> <li>Cog (by Replicate): Packaging Custom ML Models for Deployment (Custom, Dep)  </li> <li>ComfyUI: GUI for Workflow (Dev)  </li> <li>Tinygrad: Dev  </li> <li>LangChain: Dev (develop agentic AI systems)  </li> <li>CrewAI: Dev (develop agentic AI systems)</li> </ul>"},{"location":"companies/#data-platforms-and-providers","title":"Data Platforms and Providers","text":"<ul> <li>Encord: Data  </li> <li>LlamaIndex: Dev, Data Indexing  </li> <li>LAION: Data Provider, Open-Source Datasets</li> </ul>"},{"location":"companies/#end-to-end-cloud-based-ml-platforms","title":"End-to-End Cloud-Based ML Platforms","text":"<ul> <li>Google Vertex AI: API, Pre, FT, Custom, Dev, Dep, Model  </li> <li>Amazon Bedrock: API, Pre, FT, Custom, Dev, Dep, Model  </li> <li>Microsoft Azure Machine Learning: API, Pre, FT, Custom, Dev, Dep, Model  </li> <li>Databricks: API, Dev, Dep, Model  </li> <li>IBM Watson Studio: Dev, Dep, Model  </li> <li>DataRobot: Dev, Dep, Model  </li> <li>H2O.ai: Dev, Dep, Model  </li> <li>Domino Data Lab: Dev, Dep, Model  </li> <li>Algorithmia: Dev, Dep, Model</li> </ul>"},{"location":"courses/","title":"Courses","text":""},{"location":"courses/#deep-learning-in-uva","title":"Deep learning in UvA","text":"<p>For self study, the video recordings and lecture notes are provided in the Uva Deep Learning 1 Course. </p> <p>Deep Learning 1, Deep Learning 2, Computer Vision 1, Computer Vision 2 (mainly 3D computer vision or a.k.a multiview geometry) are in the DataNose, under <code>program list \\ Master Artificial Intelligent</code>.</p>"},{"location":"courses/#online-courses","title":"Online Courses","text":""},{"location":"courses/#computer-vision-deep-learning-for-computer-vision","title":"Computer Vision | Deep Learning for Computer Vision","text":"<ul> <li>Stanford CS231n: Deep Learning for Computer Vision: Videos winter 2016 and lecture notes. Deep learning based 2D computer vision. There is also MIT 6.S191: Introduction to Deep Learning and Stanford CS 25 transformers united.</li> <li>Photogrammetry I &amp; II by Cyrill Stachniss: Contains both tranditional and deep-learning based computer vision in 2D and 3D.</li> <li>Computer Vision by Andreas Geiger: Contains both tranditional and deep-learning based 2D and 3D computer vision.</li> <li>EfficientML.ai Lecture, Fall 2023, MIT 6.5940</li> </ul>"},{"location":"courses/#traditional-computer-vision","title":"Traditional Computer Vision","text":"<ul> <li>Photogrammetric Computer Vision by Cyrill Stachniss</li> <li>Multi View Geometry by Daniel Cremers: For in depth understdaning of multi view geometry, SfM (Structure from Motion), SLAM (Simultaneous Localization and Mapping). It is not a easy course. </li> </ul>"},{"location":"ddp/","title":"DDP (Distributed Data Parallel) in PyTorch","text":"<p>This manual is from the PyTorch DDP tutorial. The code can be found here. This manual summarizes the changes needed when transitioning from a single GPU to multiple GPUs on a single node, both with and without torchrun, as well as multiple GPUs across multiple nodes. You can compare the code yourself using the <code>diff</code> command:</p> <pre><code>diff --color -U 0 multigpu.py multigpu_torchrun.py\n</code></pre>"},{"location":"ddp/#single-node-multiple-gpu","title":"Single node multiple GPU","text":""},{"location":"ddp/#imports","title":"Imports","text":"<pre><code>import torch.multiprocessing as mp\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.distributed import init_process_group, destroy_process_group\nimport os\n</code></pre>"},{"location":"ddp/#script-function","title":"Script function","text":"<pre><code>def main(rank, world_size, other_args):\n    ddp_setup(rank, world_size)\n    # define dataset, model, optimizer, trainer\n    destroy_process_group()\n\n\nworld_size = torch.cuda.device_count()\nmp.spawn(main, args=(world_size, other_args,), nprocs=world_size)\n</code></pre>"},{"location":"ddp/#ddp-setup","title":"DDP setup","text":"<pre><code>os.environ[\"MASTER_ADDR\"] = \"localhost\"\nos.environ[\"MASTER_PORT\"] = \"12355\"\ninit_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\ntorch.cuda.set_device(rank)\n</code></pre>"},{"location":"ddp/#model","title":"model","text":"<pre><code>- self.model = model.to(gpu_id)\n+ self.model = DDP(model, device_ids=[gpu_id])\n</code></pre>"},{"location":"ddp/#data","title":"data","text":"<pre><code>train_data = torch.utils.data.DataLoader(\n    dataset=train_dataset,\n    batch_size=32,\n-   shuffle=True,\n+   shuffle=False,\n+   sampler=DistributedSampler(train_dataset),\n)\n</code></pre>"},{"location":"ddp/#shuffling-across-multiple-epochs","title":"Shuffling across multiple epochs","text":"<p>Calling the set_epoch() method on the DistributedSampler at the beginning of each epoch is necessary to make shuffling work properly across multiple epochs. Otherwise, the same ordering will be used in each epoch.</p> <pre><code>for epoch in epochs:\n    train_data.sampler.set_epoch(epoch)\n    for source, targets in train_data:\n</code></pre>"},{"location":"ddp/#save-checkpoints","title":"Save checkpoints","text":"<p>We only need to save model checkpoints from one process.</p> <pre><code>- ckp = model.state_dict()\n+ ckp = model.module.state_dict()\n- if epoch % save_every == 0:\n+ if gpu_id == 0 and epoch % save_every == 0:\n</code></pre>"},{"location":"ddp/#slurm-job","title":"Slurm job","text":"<pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=2\n#SBATCH --gpus=2\n#SBATCH --cpus-per-task=18\n#SBATCH --partition=gpu\n#SBATCH -o test_multigpu_%j.out\npython python_script.py arguments\n# python multigpu.py 50 10\n</code></pre>"},{"location":"ddp/#single-node-multiple-gpu-with-torchrun","title":"Single node multiple GPU with torchrun","text":""},{"location":"ddp/#script-function_1","title":"Script function","text":"<pre><code>-  world_size = torch.cuda.device_count()\n-  mp.spawn(main, args=(world_size, other_args,), nprocs=world_size)\n+  main(other_args)\n</code></pre>"},{"location":"ddp/#ddp-setup_1","title":"DDP setup","text":"<p>torchrun provided environment variables <code>os.environ[\"LOCAL_RANK\"]</code> for the GPU id:</p> <pre><code>gpu_id = int(os.environ[\"LOCAL_RANK\"])\n</code></pre> <pre><code>- os.environ[\"MASTER_ADDR\"] = \"localhost\"\n- os.environ[\"MASTER_PORT\"] = \"12355\"\n- init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n+ init_process_group(backend=\"nccl\")\n+ torch.cuda.set_device(int(os.environ[\"LOCAL_RANK\"]))\n</code></pre>"},{"location":"ddp/#slurm-job_1","title":"Slurm job","text":"<pre><code>#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=2\n#SBATCH --gpus=2\n#SBATCH --cpus-per-task=18\n#SBATCH --partition=gpu\n#SBATCH --time=00:05:00\n#SBATCH -o test_multigpu_torchrun_%j.out    \ntorchrun --nnodes=1 --nproc_per_node=2 python_script.py arguments\n# torchrun --nnodes=1 --nproc_per_node=2 multigpu_torchrun.py 50 10\n</code></pre>"},{"location":"ddp/#multi-node-multi-gpu","title":"Multi node multi GPU","text":"<p>The only diffeerence with previous one:</p> <pre><code>+ local_rank = int(os.environ[\"LOCAL_RANK\"])\n+ global_rank = int(os.environ[\"RANK\"])\n+ model = model.to(local_rank)\n\n- local_rank = int(os.environ[\"LOCAL_RANK\"]) # local_rank == gpu_id\n- model = model.to(local_rank)\n</code></pre>"},{"location":"ddp/#slurm-job_2","title":"Slurm job","text":"<pre><code>#!/bin/bash\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=4\n#SBATCH --gpus-per-node=4\n#SBATCH --cpus-per-task=18\n#SBATCH --partition=gpu\n#SBATCH --time=00:05:00\n#SBATCH -o test_multinode_%j.out  \ntorchrun --nnodes=2 --nproc_per_node=4 python_script.py arguments\n# torchrun --nnodes=2 --nproc_per_node=1 multigpu_torchrun.py 50 10\n</code></pre>"},{"location":"ddp/#references","title":"References","text":"<ul> <li>https://medium.com/pytorch/training-a-1-trillion-parameter-model-with-pytorch-fully-sharded-data-parallel-on-aws-3ac13aa96cff</li> <li>https://medium.com/pytorch/pytorch-data-parallel-best-practices-on-google-cloud-6c8da2be180d</li> <li>https://medium.com/pytorch/pytorch-sessions-at-nvidia-gtc-march-20-2023-b86210711c9b</li> </ul>"},{"location":"deep_learning_project_setup/","title":"Deep Learning Project Setup","text":""},{"location":"deep_learning_project_setup/#deep-learning-project-setup","title":"Deep Learning Project Setup","text":"<p>To establish a foundational deep learning project that includes data loading, model definition, training with validation, and hyperparameter tuning, consider the following structured approach. This setup is modular, facilitating scalability and maintenance.</p>"},{"location":"deep_learning_project_setup/#folder-structure","title":"Folder Structure","text":"<pre><code>project/\n\u2502\n\u251c\u2500\u2500 data/               # Data loading scripts\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 dataset.py      # Custom dataset script\n\u2502\n\u251c\u2500\u2500 models/             # Model architectures\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 model.py        # Model definition\n\u2502\n\u251c\u2500\u2500 configs/            # Configuration files\n\u2502   \u2514\u2500\u2500 config.yaml     # Hyperparameter configurations\n\u2502\n\u251c\u2500\u2500 scripts/            # Training and evaluation scripts\n\u2502   \u2514\u2500\u2500 train.py        # Training script\n\u2502\n\u251c\u2500\u2500 utils/              # Utility functions\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 utils.py        # Helper functions (e.g., logging, metrics)\n\u2502\n\u251c\u2500\u2500 main.py             # Entry point of the project\n\u2514\u2500\u2500 requirements.txt    # List of required packages\n</code></pre>"},{"location":"deep_learning_project_setup/#dataset-loader","title":"Dataset Loader","text":"<pre><code># data/dataset.py\nimport torch\nfrom torch.utils.data import Dataset\n\nclass SimpleDataset(Dataset):\n    def __init__(self, data, labels):\n        self.data = data\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return {\n            \"data\": torch.tensor(self.data[idx], dtype=torch.float32),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long),\n        }\n</code></pre>"},{"location":"deep_learning_project_setup/#model-definition","title":"Model Definition","text":"<pre><code># models/model.py\nimport torch.nn as nn\n\nclass SimpleModel(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(SimpleModel, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_size, 128),\n            nn.ReLU(),\n            nn.Linear(128, num_classes)\n        )\n\n    def forward(self, x):\n        return self.fc(x)\n</code></pre>"},{"location":"deep_learning_project_setup/#training-script-with-validation","title":"Training Script with Validation","text":"<pre><code># scripts/train.py\nimport torch\nfrom torch.utils.data import DataLoader, random_split\nfrom torch.optim import Adam\nfrom torch.nn import CrossEntropyLoss\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm import tqdm\nfrom data.dataset import SimpleDataset\nfrom models.model import SimpleModel\nfrom sklearn.metrics import accuracy_score\n\ndef train_model(config, dataset):\n    # Determine sizes for training and validation sets\n    total_size = len(dataset)\n    val_size = int(total_size * config['val_split'])\n    train_size = total_size - val_size\n\n    # Split the dataset\n    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\n    # Create dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n\n    # Model, Loss, Optimizer\n    model = SimpleModel(input_size=dataset[0]['data'].shape[0], num_classes=len(set(dataset.labels)))\n    criterion = CrossEntropyLoss()\n    optimizer = Adam(model.parameters(), lr=config['learning_rate'])\n\n    # TensorBoard Summary Writer\n    writer = SummaryWriter(log_dir=config['log_dir'])\n\n    # Training Loop\n    for epoch in range(config['epochs']):\n        model.train()\n        total_loss = 0\n        for batch in tqdm(train_loader):\n            data, labels = batch['data'], batch['label']\n\n            # Forward pass\n            outputs = model(data)\n            loss = criterion(outputs, labels)\n\n            # Backward pass and optimization\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n\n        avg_train_loss = total_loss / len(train_loader)\n        print(f\"Epoch [{epoch+1}/{config['epochs']}], Training Loss: {avg_train_loss:.4f}\")\n\n        # Validation\n        model.eval()\n        val_loss = 0\n        all_preds = []\n        all_labels = []\n        with torch.no_grad():\n            for batch in val_loader:\n                data, labels = batch['data'], batch['label']\n                outputs = model(data)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n                preds = torch.argmax(outputs, dim=1)\n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n\n        avg_val_loss = val_loss / len(val_loader)\n        val_accuracy = accuracy_score(all_labels, all_preds)\n        print(f\"Epoch [{epoch+1}/{config['epochs']}], Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n\n        # Log metrics to TensorBoard\n        writer.add_scalar('Training Loss', avg_train_loss, epoch + 1)\n        writer.add_scalar('Validation Loss', avg_val_loss, epoch + 1)\n        writer.add_scalar('Validation Accuracy', val_accuracy, epoch + 1)\n\n    writer.close()\n</code></pre>"},{"location":"deep_learning_project_setup/#main-entry","title":"Main Entry","text":"<pre><code># main.py\nimport yaml\nimport numpy as np\nfrom data.dataset import SimpleDataset\nfrom scripts.train import train_model\n\nif __name__ == \"__main__\":\n    # Load configurations\n    with open(\"configs/config.yaml\", \"r\") as file:\n        config = yaml.safe_load(file)\n\n    # Dummy data (replace with real data loading)\n    X = np.random.rand(1000, 10)\n    y = np.random.randint(0, 2, 1000)\n\n    # Create dataset\n    dataset = SimpleDataset(X, y)\n\n    # Train the model\n    train_model(config, dataset)\n</code></pre>"},{"location":"deep_learning_project_setup/#configuration","title":"Configuration","text":"<p>Ensure your configuration file includes the <code>log_dir</code> for TensorBoard logs and a <code>val_split</code> parameter to define the proportion of data used for validation.</p> <pre><code># configs/config.yaml\nbatch_size: 16\nlearning_rate: 0.001\nepochs: 10\nlog_dir: 'runs/experiment_1'\nval_split: 0.2  # 20% of data used for validation\n</code></pre>"},{"location":"deep_learning_project_setup/#running-the-code","title":"Running the Code","text":"<ol> <li> <p>Install Dependencies:    <code>bash    pip install -r requirements.txt</code></p> </li> <li> <p>Run the Training:    <code>bash    python main.py</code></p> </li> <li> <p>Launch TensorBoard:    <code>bash    tensorboard --port 4004 --logdir=runs</code>    Open the provided URL in your browser to access the TensorBoard dashboard.</p> </li> </ol> <p>By integrating PyTorch's <code>random_split</code> function, you can effectively partition your dataset into training and validation sets, facilitating model evaluation without the need for external libraries. </p>"},{"location":"deep_learning_project_setup/#simplified-hyperparameter-tuning","title":"Simplified Hyperparameter Tuning","text":"<p>For a more concise approach to hyperparameter tuning, consider using a configuration file to define multiple sets of hyperparameters and iterate over them. Here's an example:</p>"},{"location":"deep_learning_project_setup/#define-hyperparameter-sets","title":"Define Hyperparameter Sets:","text":"<p>Create a YAML file listing different hyperparameter configurations:</p> <pre><code># configs/hyperparams.yaml\nexperiments:\n    - batch_size: 16\n    learning_rate: 0.001\n    epochs: 5\n    log_dir: 'runs/exp1'\n    - batch_size: 32\n    learning_rate: 0.01\n    epochs: 10\n    log_dir: 'runs/exp2'\n</code></pre>"},{"location":"deep_learning_project_setup/#modify-the-main-script","title":"Modify the Main Script:","text":"<p>Update your <code>main.py</code> to load and iterate over these configurations:</p> <pre><code># main.py\nimport yaml\nfrom scripts.train import train_model\n\nif __name__ == \"__main__\":\n    # Load hyperparameter configurations\n    with open(\"configs/hyperparams.yaml\", \"r\") as file:\n        experiments = yaml.safe_load(file)['experiments']\n\n    # Iterate over each configuration\n    for idx, config in enumerate(experiments):\n        print(f\"Running experiment {idx + 1}/{len(experiments)} with config: {config}\")\n        train_model(config)\n</code></pre> <p>This approach allows you to manage multiple experiments efficiently, with each configuration's results logged separately for easy comparison.</p>"},{"location":"deep_learning_project_setup/#references","title":"References:","text":"<ul> <li>How to use TensorBoard with PyTorch</li> <li>Visualizing Models, Data, and Training with TensorBoard </li> </ul>"},{"location":"docker/","title":"DOCKER","text":""},{"location":"docker/#installation-for-ubuntu","title":"Installation for Ubuntu","text":"<p>More information can be found on the \"engine install ubuntu\".</p> <pre><code># Add Docker's official GPG key:\nsudo apt-get update\nsudo apt-get install ca-certificates curl\nsudo install -m 0755 -d /etc/apt/keyrings\nsudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\nsudo chmod a+r /etc/apt/keyrings/docker.asc\n\n# Add the repository to Apt sources:\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\n  $(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\") stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\n\n# Install the latest version\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> <p>More information can be found on the linux postinstall.</p> <pre><code># Create the `docker` group. The Docker group already exists, so there is no need to run this command.\nsudo groupadd docker\n\n# Add your user to the docker group.\nsudo usermod -aG docker $USER\n\n# Activate the changes to groups. \n# I think it is better to restart the system than to run this command. I encountered a weird issue when I installed Docker Compose.\nnewgrp docker\n\n# Verify docker\ndocker run hello-world\n</code></pre>"},{"location":"docker/#uninstall-docker-engine","title":"Uninstall Docker Engine","text":"<p>More information can be found on the official website.</p> <pre><code>sudo apt-get purge docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin docker-ce-rootless-extras\n\nsudo rm -rf /var/lib/docker\nsudo rm -rf /var/lib/containerd\n\nsudo rm /etc/apt/sources.list.d/docker.list\nsudo rm /etc/apt/keyrings/docker.asc\n</code></pre>"},{"location":"docker/#basic-commands","title":"Basic Commands","text":"<p>Here's a guide to essential Docker commands, followed by a cheat sheet for quick reference.</p>"},{"location":"docker/#listing-docker-images-and-containers","title":"Listing Docker Images and Containers","text":""},{"location":"docker/#list-images","title":"List Images:","text":"<p>To view all Docker images on your system:</p> <pre><code>docker images\n</code></pre> <p>This displays a table of images with details like repository, tag, and image ID.</p>"},{"location":"docker/#list-running-containers","title":"List Running Containers:","text":"<p>To see all currently running containers:</p> <pre><code>docker ps\n</code></pre> <p>This shows a list of active containers with their IDs, names, and statuses.</p>"},{"location":"docker/#list-all-containers","title":"List All Containers","text":"<p>To list all containers, including those that are stopped:</p> <p><code>bash   docker ps -a</code>   This provides a comprehensive list of all containers, regardless of their state.</p>"},{"location":"docker/#managing-docker-images","title":"Managing Docker Images","text":""},{"location":"docker/#pull-an-image","title":"Pull an Image","text":"<p>To download an image from Docker Hub:</p> <pre><code>docker pull &lt;image_name&gt;\n</code></pre> <p>Replace <code>&lt;image_name&gt;</code> with the desired image, e.g., <code>ubuntu</code>.</p>"},{"location":"docker/#remove-an-image","title":"Remove an Image","text":"<p>To delete a specific image:</p> <pre><code>docker rmi &lt;image_name_or_id&gt;\n</code></pre> <p>Use the image name or ID from the <code>docker images</code> list.</p>"},{"location":"docker/#building-docker-images","title":"Building Docker Images","text":""},{"location":"docker/#build-an-image-from-a-dockerfile","title":"Build an Image from a Dockerfile","text":"<p>To create a Docker image from a Dockerfile:</p> <pre><code>docker build -t &lt;image_name&gt; &lt;path_to_dockerfile_directory&gt;\n</code></pre> <p>Replace <code>&lt;image_name&gt;</code> with your desired image name and <code>&lt;path_to_dockerfile_directory&gt;</code> with the path to the directory containing your Dockerfile. The <code>-t</code> flag tags the image with a name.</p> <p>Example:</p> <pre><code>docker build -t myapp:latest .\n</code></pre> <p>This builds an image named <code>myapp</code> with the tag <code>latest</code> from the Dockerfile in the current directory.</p>"},{"location":"docker/#running-and-managing-containers","title":"Running and Managing Containers","text":""},{"location":"docker/#run-a-container","title":"Run a Container","text":"<p>To create and start a new container:</p> <pre><code>docker run [OPTIONS] &lt;image_name&gt;\n</code></pre> <p>Common options include: - <code>-d</code>: Run the container in detached mode (in the background). - <code>-it</code>: Run the container in interactive mode with a terminal. - <code>--name &lt;container_name&gt;</code>: Assign a name to the container. - <code>-p &lt;host_port&gt;:&lt;container_port&gt;</code>: Map host port to container port. - <code>-v &lt;host_directory&gt;:&lt;container_directory&gt;</code>: Mount a host directory as a volume in the container.</p> <p>Example:</p> <pre><code>docker run -it bird-behavior bash\n</code></pre> <p>Example:</p> <p>This runs a container named <code>bird-behavior</code> and opens an interactive bash shell inside the <code>bird-behavior</code> container.</p> <pre><code>docker run -d -p 8080:80 -v /host/data:/container/data --name mynginx nginx\n</code></pre> <p>This runs an Nginx container named <code>mynginx</code> in detached mode, mapping port 8080 on the host to port 80 in the container, and mounts the host directory <code>/host/data</code> to <code>/container/data</code> in the container.</p>"},{"location":"docker/#execute-commands-in-a-running-container","title":"Execute Commands in a Running Container","text":"<p>To run a command inside a running container:</p> <pre><code>docker exec [OPTIONS] &lt;container_name_or_id&gt; &lt;command&gt;\n ```\n\nCommon options:\n- `-it`: Run in interactive mode with a terminal.\n\nExample:\n\n```bash\ndocker exec -it mynginx /bin/bash\n</code></pre> <p>This opens an interactive bash shell inside the <code>mynginx</code> container.</p>"},{"location":"docker/#stop-a-running-container","title":"Stop a Running Container","text":"<p>To stop a container:</p> <pre><code>docker stop &lt;container_name_or_id&gt;\n</code></pre>"},{"location":"docker/#start-a-stopped-container","title":"Start a Stopped Container","text":"<p>To start a container that has been stopped:</p> <pre><code>docker start &lt;container_name_or_id&gt;\n</code></pre>"},{"location":"docker/#remove-a-container","title":"Remove a Container","text":"<p>To delete a container:</p> <pre><code>docker rm &lt;container_name_or_id&gt;\n</code></pre> <p>Note: Ensure the container is stopped before removing it.</p>"},{"location":"docker/#viewing-logs-and-inspecting-containers","title":"Viewing Logs and Inspecting Containers","text":""},{"location":"docker/#view-container-logs","title":"View Container Logs","text":"<p>To see the logs of a container:</p> <pre><code>docker logs &lt;container_name_or_id&gt;\n</code></pre>"},{"location":"docker/#inspect-container-details","title":"Inspect Container Details","text":"<p>To get detailed information about a container:</p> <pre><code>docker inspect &lt;container_name_or_id&gt;\n</code></pre>"},{"location":"docker/#docker-compose","title":"Docker Compose","text":"<p>Docker Compose is a tool that simplifies running applications with multiple containers. By defining services, networks, and volumes in a single YAML file, you can start and manage all components of your application with one command. </p> <p>You can find an example of a Docker Compose file here. The description is provided below.</p>"},{"location":"docker/#creating-a-docker-composeyml-file","title":"Creating a <code>docker-compose.yml</code> File","text":"<p>Create a <code>docker-compose.yml</code> file with the following content:</p> <pre><code>version: '3'\nservices:\n  jekyll:\n    image: jekyll/jekyll:latest\n    command: jekyll serve --watch --incremental\n    ports:\n      - \"4000:4000\"\n    volumes:\n      - .:/srv/jekyll\n</code></pre>"},{"location":"docker/#running-the-docker-compose-file","title":"Running the Docker Compose File","text":"<p>To start the services defined in your <code>docker-compose.yml</code> file, run:</p> <pre><code>docker-compose up\n</code></pre>"},{"location":"docker/#cheat-sheet","title":"Cheat Sheet","text":"Command Description <code>docker images</code> List all Docker images <code>docker ps</code> List running containers <code>docker ps -a</code> List all containers (running and stopped) <code>docker pull &lt;image_name&gt;</code> Pull an image from Docker Hub <code>docker rmi &lt;image_name_or_id&gt;</code> Remove a Docker image <code>docker build -t &lt;image_name&gt; &lt;path&gt;</code> Build an image from a Dockerfile <code>docker run [OPTIONS] &lt;image_name&gt;</code> Run a new container <code>docker exec [OPTIONS] &lt;container&gt; &lt;command&gt;</code> Execute a command in a running container <code>docker stop &lt;container_name_or_id&gt;</code> Stop a running container <code>docker start &lt;container_name_or_id&gt;</code> Start a stopped container <code>docker rm &lt;container_name_or_id&gt;</code> Remove a container <code>docker logs &lt;container_name_or_id&gt;</code> View logs of a container <code>docker inspect &lt;container_name_or_id&gt;</code> Inspect detailed information of a container"},{"location":"git/","title":"Git","text":""},{"location":"git/#git-basics-a-simple-manual","title":"Git Basics: A Simple Manual","text":"<p>Git is a version control system that helps you manage your code and collaborate with others. This manual will cover some basic Git commands to get you started.</p>"},{"location":"git/#setting-up-git","title":"Setting Up Git","text":"<ul> <li>Install Git on your computer. You can download it from here and follow the installation instructions.</li> </ul>"},{"location":"git/#configuring-git","title":"Configuring Git","text":"<ul> <li>After installation, set your name and email using the following commands:</li> </ul> <pre><code>git config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n</code></pre>"},{"location":"git/#creating-a-new-repository","title":"Creating a New Repository","text":"<ul> <li>Local Repository: To start a new project with Git, create a new directory and navigate into it. Then run:</li> </ul> <pre><code>git init\n</code></pre> <p>This step is better to be done on GitHub.</p> <ul> <li>Clone Repository: To work on an existing project, clone it from a remote repository using:</li> </ul> <pre><code>git clone &lt;remote_url&gt;\n</code></pre> <ul> <li>Switching Branches: Create and switch to a new branch with:</li> </ul> <pre><code>git checkout -b &lt;branch_name&gt;\n</code></pre>"},{"location":"git/#basic-commands","title":"Basic Commands","text":"<ul> <li>Adding Files: Stage changes for commit using:</li> </ul> <pre><code>git add &lt;filename&gt;\n</code></pre> <ul> <li>Committing Changes: Save staged changes to your local repository with:</li> </ul> <pre><code>git commit -m \"Your commit message\"\n</code></pre> <p>Use <code>-a</code> to tell the command to automatically stage files that have been modified and deleted. So, you don't need to use <code>git add</code> anymore. For new files, <code>git add</code> should be used.</p> <pre><code>git commit -am \"Your commit message\"\n</code></pre>"},{"location":"git/#collaboration","title":"Collaboration","text":"<ul> <li>Pushing Changes: Upload your local commits to a remote repository using:</li> </ul> <pre><code>git push origin &lt;branch_name&gt;\n</code></pre> <ul> <li>Pulling Changes: Retrieve and merge changes from the remote repository with:</li> </ul> <pre><code>git pull origin &lt;branch_name&gt;\n</code></pre>"},{"location":"git/#view-commands","title":"View Commands","text":"<ul> <li>Checking Status: To see which files are staged or modified, use:</li> </ul> <pre><code>git status\n</code></pre> <ul> <li>Viewing Commit History: See a list of past commits using:</li> </ul> <pre><code>git log\n</code></pre>"},{"location":"git/#git-advanced","title":"Git Advanced","text":""},{"location":"git/#git-lfs-large-file-storage-guide","title":"Git LFS (Large File Storage) Guide","text":"<p>Git LFS is a tool that helps manage large files in Git repositories efficiently. It stores large files (like images, videos, etc.) outside the repository while keeping references to them in your version control system. This reduces the size of your Git repository and optimizes performance.</p>"},{"location":"git/#step-1-install-git-lfs","title":"Step 1: Install Git LFS","text":"<p>To begin using Git LFS, you need to install it on your system:</p> <pre><code>sudo apt install git-lfs\n</code></pre>"},{"location":"git/#step-2-initialize-git-lfs-in-your-repository","title":"Step 2: Initialize Git LFS in Your Repository","text":"<p>After installation, you need to enable Git LFS in your project:</p> <pre><code>git lfs install\n</code></pre> <p>Now, your Git repository is ready to handle large files.</p>"},{"location":"git/#step-3-track-large-files","title":"Step 3: Track Large Files","text":"<p>Specify which file types or individual large files you want Git LFS to track.</p> <ul> <li>Track by Extension: To track all files with a certain extension (e.g., <code>.psd</code>), use:</li> </ul> <pre><code>git lfs track \"*.psd\"\n</code></pre> <p>This will automatically add a line in the <code>.gitattributes</code> file, ensuring all <code>.psd</code> files are managed by Git LFS:</p> <pre><code>*.psd filter=lfs diff=lfs merge=lfs -text\n</code></pre> <ul> <li>Track Individual Files: For large files without a specific extension, you can track them individually by specifying the exact file path:</li> </ul> <pre><code>git lfs track \"my_large_file\"\n</code></pre> <p>This will add an entry to <code>.gitattributes</code> like:</p> <pre><code>my_large_file filter=lfs diff=lfs merge=lfs -text\n</code></pre> <p>Note: Adding text manually in the <code>.gitattributes</code> is the same as running <code>git lfs track</code>. </p> <p>By tracking files this way, Git LFS ensures large files are stored separately while keeping references in your Git repository.</p> <p>Once a file is tracked by Git LFS and pushed to the remote repository, Git stores a reference (pointer) to the large file in your regular Git repository. The actual large file is stored separately on the Git LFS server (which could be GitHub or another remote LFS storage). </p> <p>In the future, when you push or pull, Git will just handle the reference (which is small in size) in the main repository, while Git LFS takes care of storing or fetching the actual large file.</p>"},{"location":"git/#step-4-add-commit-and-push-files-to-the-remote-repository","title":"Step 4: Add, Commit and Push Files to the Remote Repository","text":"<p>Now, add the large files you want to commit to your repository as you normally would:</p> <pre><code>git add .gitattributes\ngit add large_file.psd\ngit commit -m \"Add large file using Git LFS\"\ngit push\n</code></pre>"},{"location":"git/#step-5-pull-files-from-a-remote-repository","title":"Step 5: Pull Files from a Remote Repository","text":"<p>When you or someone else pulls the project, Git LFS will automatically download the large files referenced in the repository:</p> <pre><code>git pull\n</code></pre> <p>If you only pull the references to the large files, you can use the following command to explicitly download the large files:</p> <pre><code>git lfs pull\n</code></pre> <p>When using <code>git clone</code>, all files, including those tracked by LFS, are downloaded. If LFS-tracked files have been updated, running <code>git lfs pull</code> will download only the updated large files. <code>git lfs push</code> can be used to push only the large files to the LFS server without pushing other changes to the repository. However, in most cases, just running <code>git push</code> is sufficient, as Git LFS automatically handles the large files.</p>"},{"location":"git/#step-6-check-git-lfs-status","title":"Step 6: Check Git LFS Status","text":"<p>To check the current status of tracked files in Git LFS, use:</p> <pre><code>git lfs status\n</code></pre> <p>This command shows you which files are being tracked and which are pending to be pushed or committed.</p>"},{"location":"git/#links","title":"Links","text":"<ul> <li>More information on Git from coderefinery</li> </ul>"},{"location":"gpu/","title":"Access Snellius GPUs","text":""},{"location":"gpu/#small-compute-via-fnwi-faculty-and-nwo","title":"Small Compute via FNWI Faculty and NWO","text":"<p>The FNWI institute offers small compute resources about three times a year, with each allocation providing approximately 50K-100K SBUs. NWO also provides access to small compute resources.</p> <p>Create a ticket at https://servicedesk.surf.nl under \"Apply for access / Direct institute contract\" or \"Apply for access / Small Compute applications (NWO).\"</p> <p>Follow the instructions in the Setup section to create an account.</p>"},{"location":"gpu/#nwo-large-compute","title":"NWO Large Compute","text":"<p>For larger amounts of compute, please refer to possible options in NWO grants or Access to compute services. </p>"},{"location":"gpu/#research-capacity-computing-services-rccs","title":"Research Capacity Computing Services (RCCS)","text":"<p>For more information, visit RCSS: Research Capacity Computing Services. To find the latest rates for services, search for \"SURF Services and Rates\" on Google.</p>"},{"location":"gpu/#setup","title":"Setup","text":"<p>Create an account</p> <p>https://portal.cua.surf.nl : first copied public key in here (only done once)</p> <p>In the case of an issue, create in https://servicedesk.surf.nl a ticket under \"Servicedesk / create a ticket\" or email servicedesk@surf.nl.  Usage</p> <p>Use the Snellius (similar for e.g. sshfs/scp):</p> <pre><code>ssh -X username@snellius.surf.nl\n</code></pre>"},{"location":"gpu/#setup-environment","title":"Setup environment","text":"<p>The first time to setup your environment, run below script:</p> <pre><code>module purge # unload all modules\nmodule load 2022\nmodule load Anaconda3/2022.05 # version 4.12.0\nconda init bash\n</code></pre> <p>After that, the basic virtualenv from conda can be created. See below e.g.:</p> <pre><code>conda create -n test python=3.8\nconda activate test\npip install torch # version 2.0.0+cu117\n</code></pre>"},{"location":"gpu/#schedule-tasks","title":"Schedule Tasks","text":"<p>SLURM is a job scheduler used by many computer clusters and supercomputer, such as Snellius. It allocates resources to users and monitors work. It is a configurable workload manager. <code>squeue</code>, <code>sbatch</code>, <code>srun</code>, <code>sinfo</code>, and <code>scancel</code> are examples of the most commonly used commands in SLURM.</p>"},{"location":"gpu/#use-gpus","title":"Use GPUs","text":""},{"location":"gpu/#remote-vscode","title":"Remote VSCode","text":"<ul> <li>Install the <code>Remote-SSH</code> extension from the Extensions view.</li> <li>Run <code>Remote-SSH: Connect to Host</code> to connect to a remote host.  </li> <li>If you don\u2019t have a <code>~/.ssh/config</code> file set up, you\u2019ll need to run <code>Remote-SSH: Add New SSH Host</code> or set it up manually.</li> </ul> <p>If you want to access the GPU machine (e.g., <code>gcn700</code>), you can directly connect via Snellius using the command <code>ssh gcn700</code>. However, if you want to debug using Visual Studio Code (VSCode) on your computer, you need to add the following lines to your <code>~/.ssh/config</code> file. The rest of the process is the same as using remote SSH. Note that you can use <code>ssh me</code>, <code>ssh gcn700</code>, or remote SSH in VSCode.</p> <pre><code>Host me\n    User myuser\n    HostName snellius.surf.nl\n    IdentityFile ~/.ssh/id_rsa\nHost gcn700\n    User myuser\n    HostName gcn700\n    ProxyJump me\n    IdentityFile ~/.ssh/id_rsa\n</code></pre>"},{"location":"gpu/#quick-test","title":"Quick Test","text":"<p>The <code>gcn1</code> node is available for quick GPU tests without the need to request a GPU. However, this node has limited resources. For reference examples, check out Using IDEs and Remote Visualization.</p>"},{"location":"gpu/#run-a-job","title":"Run a job:","text":"<p>There is a web-based interface to request resources for interactive session via this link. However, I recommend using a SLURM job instead; see below for details.</p> <p>NB. The run file should be executable. Make it executable with <code>chmod a+x runfile.sh</code>.</p> <pre><code>sbatch runfile.sh\n</code></pre> <p>e.g. runfile.sh:</p> <pre><code>#!/bin/bash\n#SBATCH --gpus=1\n#SBATCH --partition=gpu\n#SBATCH --time=14:00:00\n#SBATCH -o yolo8_train4_%j.out\n\necho \"gpus $SLURM_GPUS on node: $SLURM_GPUS_ON_NODE\"\necho \"nodes nnodes: $SLURM_NNODES, nodeid: $SLURM_NODEID, nodelist $SLURM_NODELIST\"\necho \"cpus on node: $SLURM_CPUS_ON_NODE per gpu $SLURM_CPUS_PER_GPU per task $SLURM_CPUS_PER_TASK omp num thread $OMP_NUM_THREADS\"\necho \"tasks per node $SLURM_TASKS_PER_NODE pid $SLURM_TASK_PID\"\n\n# activate your environment\nsource $HOME/.bashrc\nconda activate test # your conda venv\n\necho \"start training\"\nyolo detect train data=/home/username/data/data8_v1/data.yaml model=/home/username/exp/runs/detect/bgr23/weights/best.pt imgsz=1920 batch=8 epochs=100 name=bgr cache=true close_mosaic=0 augment=True rect=False mosaic=1.0 mixup=0.0\necho \"end training\"\n</code></pre> <p>More SBATCH options and the \"output environmental variables\" can be found from the sbatch help. </p> <p></p> <p>Check job is running</p> <p>User squeue with job id or username. </p> <pre><code>squeue -j jobid\nsqueue -u username\n# squeue with more options\nsqueue -o \"%.10i %.9P %.25j %.8u %.8T %.10M %.9l %.6D %.10Q %.20S %R\"\n</code></pre> <p>If the job is running, it will save the result in the output file with the name specified by <code>SBATCH -o</code> option. NB. <code>%j</code> in the name replaced by job id. In the example <code>yolo8_train4_%j.out</code>, the output file will be olo8_train4_2137977.out. The job id is the id you get after running sbatch.</p> <p>IMPORTANT Each person has a limited budget in the unit of SBU (system billing unit). It is also listed as accounting weight factor in Snellius partitions and accounting. e.g. If you request for 1 A100 GPU, it is 1/4 node, which has 18 cores for 10 hours, the SBU is <code>128 * 10 = 1280</code>. So in a <code>runfile.sh</code>, the basic slurm settings are as:</p> <pre><code>#SBATCH --gpus=1\n#SBATCH --partition=gpu\n#SBATCH --time=14:00:00\n</code></pre> <p>NB. <code>--cpus-per-gpu</code> or <code>--cpus-per-task</code> is automatically set for 1/4 of node, which in <code>gpu</code> partition is 18. For more info, check SBU calculating.</p> <p></p> <p>There are more options for variables such as below. You can get the full list from the sbatch help. </p> <pre><code>#SBATCH --gpus-per-node=1\n#SBATCH --cpus-per-gpu=18\n#SBATCH --cpus-per-task=1\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n</code></pre> <p>NB: Jobs that require more resources and or running for a long time (walltime: <code>SBATCH --time</code>) are not easily scheduled. Try to first test if everything is OK by running one or two epochs, then request for resources. Moreover, estimate the time your experiment runs by roughly calculating how long each epoch takes and multiply by epochs and then increase this time for a bit counting for caching, data transfer.</p> <p></p> <p>Check finished jobs</p> <pre><code>sacct -j jobid -o \"JobID,JobName,MaxRSS,Elapsed,NNodes,NodeList\"\n</code></pre> <p>More options are in the sacct help page.</p> <p></p> <p>Run tensorboard from remote computer Connect to Snellius and map your local port to remote port:</p> <pre><code>ssh -X username@snellius.surf.nl -L your_local_port:127.0.0.1:remote_port\n</code></pre> <p>In Snellius machine, run tensorboard with the remote port:</p> <pre><code>tensorboard --logdir_spec=18:/home/username/exp18,12:/home/username/exp12 --port remote_port # remote_port = 60011\n</code></pre> <p>Now, in the local machine, run <code>http://localhost:local_port</code>, e.g. <code>http://localhost:36999</code>. </p> <p></p> <p>Interactive mode</p> <p>IMPORTANT: This part is not recommended. Use it if you want to have a short check and want to use a bash.</p> <p>This is same as sbatch with runfile.sh but parameters are set in srun. In this way, the bash will be active and you are in the machine.</p> <pre><code>srun --gpus=1 --partition=gpu --time=00:10:00 --pty bash -il\n</code></pre> <p>Multigpu in single or multi node For every GPU there is 18 CPU no matter if specified in slurm or not. Slurm, batch scheduler, will ignore if another value for CPU is specified. Each task runs on one CPU. So <code>ntasks-per-node</code> or <code>ntasks</code> are the same here. Apparently, <code>with OMP_NUM_THREADS=4</code>, or other value, we can tell torchrun to use 4 threads per CPU.</p> <p>Basically, only specifiying number of gpus and the partition is enough. Below example uses 2 GPUs on a single Node, with 1 threads per CPU. </p> <pre><code>#!/bin/bash\n#SBATCH --gpus=2\n#SBATCH --partition=gpu\n#changing the  OMP_NUM_THREADS env variable is the same as --cpus-per-task\nexport OMP_NUM_THREADS=1\ntorchrun --node_rank=0 --nnodes=1 --nproc_per_node=2 ~/test/multigpu_torchrun.py 50 10\n</code></pre> <p>For more information on ddp (distributed data parallel) in pytorch, look at the tutorial.</p> <p>Wandb in Snellius</p> <p>First run <code>wandb init</code> before sending the job via sbatch. Then run the code which has <code>wandb.init(project=project_name)</code>. <code>Project_name</code> is wandb project.</p> <p>Useful commands</p> <ul> <li><code>htop</code>, <code>nvtop</code>: monitor CPU and GPU respectively</li> <li><code>sbatch</code>: run a job</li> <li><code>srun</code>: run a job. e.g. run job in the interactive mode: <code>srun --gpus=1 --partition=gpu --time=00:10:00 --pty bash -il</code> </li> <li><code>salloc</code>: allocate resources interactively and then run a command. e.g <code>salloc --gpus=1 --partition=gpu --time=00:10:00</code></li> <li><code>squeue</code>: show the status of the job</li> <li><code>scancel</code>: cancel the job. </li> <li><code>scontrol</code>: show detailed job information. e.g. show job detail: <code>scontrol show j 7605565</code></li> <li><code>sinfo</code>: get information about GPUs. e.g. <code>\"%9P %70N %5t %32G %6D\"</code>, <code>sinfo -e -o  \"%9P %.6D %10X %4Y %24N %24f %32G\"</code>, <code>sinfo -p gpu</code></li> <li><code>sacct</code>: get statistics on completed jobs</li> <li><code>accinfo</code>, <code>budget-overview -p gpu</code>, <code>accuse</code>: show how much credite is left (Snellius commands)</li> <li><code>myquota</code>: show the limit of files. They are also listed in Snellius hardware and file systems.</li> <li><code>gpustat -acp</code>: show the gpu usage. It should be installed with pip, <code>pip install gpustat</code>. It has the information from <code>nvidia-smi</code>, but one-liner. </li> <li><code>module load/unload/purge/list/display/avail</code>: <ul> <li><code>load/unload/purge</code>: e.g. <code>module load CUDA/11.8.0</code>: load this module and use <code>unload</code> to unload this module. <code>purge</code> unload all modules.</li> <li><code>list</code>: e.g. <code>module list</code>: list of loaded modules. </li> <li><code>display</code>: e.g. <code>module display CUDA/11.8.0</code>: show information on where this module is. </li> <li><code>avail</code>: e.g. <code>module avail</code>: show list of all available modules, but first load 2022/2023/or higher version if available. </li> </ul> </li> </ul> <p>Some examples are given in Convenient Slurm commands. </p> <p></p>"},{"location":"gpu/#useful-links","title":"Useful links:","text":"<ul> <li>SURF service desk portal</li> <li>SURF wiki</li> <li>Snellius hardware </li> <li>file systems</li> <li>SBU calculating</li> <li>Example job scripts</li> <li>Convenient Slurm commands</li> <li>Squeue help: just use <code>squeue --help</code></li> <li>uvadlc: Working with the Snellius cluster</li> <li>microhh</li> </ul>"},{"location":"gpu/#details","title":"Details","text":""},{"location":"gpu/#slrum-interactive-mode","title":"SLRUM interactive mode","text":"<p>In the interactive mode, you can see slurm variables. E.g. <code>echo $$SLURM_MEM_PER_CPU</code>, <code>echo $SLURM_CPUS_PER_TASK</code>.</p> <pre><code># interactive mode: request 2 CPUs (-c,--cpus-per-task), time can be unlimited (time=UNLIMITED)\nsrun -c2 --mem-per-cpu=200G --pty bash -il\n# interactive mode: request 1 GPU\nsrun --gpus=1 --partition=gpu --time=00:10:00 --pty bash -il\n</code></pre>"},{"location":"gpu/#slurm-list-of-common-commands","title":"SLURM list of common commands","text":"<pre><code>#SBATCH --job-name=result # appears in squeue\n#SBATCH -o exps/test_%j.out # -o,--output, %j=job id\n#SBATCH --error=test_%j.err\n#SBATCH --time=00:10:00 # time can be UNLIMITED\n#SBATCH --mem-per-cpu=1G\n#SBATCH --cpus-per-task=2 # -c,--cpus-per-task\n#SBATCH --gpus=1 # number of gpu\n#SBATCH --partition=gpu # type of gpu\n</code></pre> <p>Only these values are shown on snellius:</p> <pre><code>$SLURM_CPUS_ON_NODE:  the number of CPUs allocated to your job using the following environment variable:\n$SLURM_GPUS: This gives you the total number of GPUs allocated to your job.\n</code></pre> <p>These values are empty on snellius:</p> <pre><code>$SLURM_MEM_PER_NODE: This variable gives you the total amount of memory allocated per node in megabytes (MB)\n$SLURM_MEM_PER_CPU * $SLURM_CPUS_ON_NODE: in the case of requesting memory using --mem-per-cpu\n\n$SLURM_GPUS_PER_NODE: This gives the number of GPUs per node allocated to your job.\n$SLURM_JOB_GPUS: This gives you a list of the GPUs allocated to your job.\n\necho \"cpu pertask: $SLURM_CPUS_PER_TASK\"\necho \"cpu per gpu: $SLURM_CPUS_PER_GPU\"\n</code></pre>"},{"location":"gpu/#slurm-task-array","title":"SLURM task array","text":"<p>Example using task array:</p> <pre><code>#SBATCH --output=test_%A_%a.out # -o,--output, %A=job id, %a=array number\n#SBATCH --array=0-1\n\npython your_script.py $SLURM_ARRAY_TASK_ID\n</code></pre>"},{"location":"gpu/#get-cpu-and-gpu-information","title":"Get CPU and GPU information","text":"<p>When the job is running, you can get number of GPU, CPU, RAM.</p> <pre><code># job_id = 7605565\n`scontrol show job 7605565`\n</code></pre>"},{"location":"gpu/#cpu","title":"CPU","text":"<pre><code># RAM\n# ---\n# python\nf\"{psutil.virtual_memory().total:,}\" # total of the node\n# command line\nfree -h # total of the node\nhtop -u -d 0 # total of the node\n\n# disk space\n# ----------\nf\"{psutil.disk_usage('/').total:,}\" # total of the node\n# command line\ndf -h --total # total of the node\n\n# Number of CPUs\n# --------------\n# python\nlen(os.sched_getaffinity(0))\n# N.B: total cpu (72 for A100 not divided by 4)\npsutil.cpu_count(logical=True) # total of the node\nos.cpu_count() # total of the node\n# command line\nnproc\nhtop -u -d 0 # total of the node\n</code></pre>"},{"location":"gpu/#gpu","title":"GPU","text":"<pre><code># GPU VRAM\n# --------\n# python\nf\"{torch.cuda.get_device_properties(0).total_memory:,}\"\n# command line\nnvidia-smi --query-gpu=memory.total --format=csv\n\n# GPU type\n# --------\ntorch.cuda.get_device_name(0) \n# command line\nnvidia-smi -L\n\n# Number of GPUs\n# --------------\n# python\ntorch.cuda.device_count()\n# command line\nnvidia-smi -L\n</code></pre>"},{"location":"gpu/#sinfo","title":"Sinfo","text":"<p>Status of the nodes:</p> <pre><code>sinfo -o \"%9P %70N %5t %32G %6D\"\n</code></pre> <pre><code>    %P is the partition name.\n    %N is the node name.\n    %t is the state of the node (e.g., idle, allocated, down).\n    %G is the generic resource information, including GPUs.\n    %D is the number of nodes.\n</code></pre> <p>You can get detailed node information via: <code>scontrol show node &lt;node-name&gt;</code></p> <pre><code>scontrol show nodes | awk '/NodeName/ {node=$1} /State=/ {state=$1} /Gres=gpu/ {print node, state}'\n</code></pre> <p>Get other information:</p> <pre><code>&gt; sinfo -e -o \"%9P %.6D %10X %4Y %24N %24f %32G\" | grep gpu_a\ngpu_a100      36 4          18   gcn[37-72]               hwperf,scratch-node      gpu:a100:4(S:0-1),cpu:72\n</code></pre> <p>The <code>-e</code> option in sinfo stands for \"extend\" and is used to show information about all nodes, even those that are in states like down, drain, fail, or unknown. Without -e, sinfo typically only shows nodes that are available or idle.</p> <p>Breaking down each field:</p> <pre><code>    %9P: Partition name, gpu_a100.\n    %6D: Number of nodes, 36.\n    %10X: Number of GPUs (sockets) per node, 4.\n    %4Y:  Number of cores per GPU (socket), 18.\n    %24N: List of nodes in this partition (in this case, gcn[37-72]).\n    %24f: Features of the nodes (e.g., hwperf,scratch-node).\n    %32G: Shows detailed information about GPU and CPU availability (e.g., gpu:a100:4(S:0-1),cpu:72).\n</code></pre> <pre><code>&gt; sinfo | grep gpu_a\n\ngpu_a100     up 5-00:00:00      5   resv gcn[47-49,69,71]\n</code></pre> <p>Explanation:</p> <pre><code>    gpu_a100 is the partition name.\n    up 5-00:00:00 means the partition is up and available for 5 days.\n    5 represents the number of nodes available.\n    resv indicates that the nodes are reserved.\n    gcn[47-49,69,71] specifies the nodes within the gpu_a100 partition.\n</code></pre>"},{"location":"gpu/#free-gpus","title":"Free GPUs","text":"<p>Here is a list of free GPUs:  </p> <ul> <li>Google Colab</li> <li>Kaggle</li> <li>Gradient by Paperspace</li> <li>Amazon SageMaker Studio Lab</li> <li>Microsoft Azure (for student accounts)</li> </ul>"},{"location":"gpu/#external-gpus","title":"External GPUs","text":"<ul> <li>Cloud GPU comparison</li> <li>Vast.ai</li> <li>Lambda Labs: On demand, One, two &amp; three year contracts.</li> <li>runpod.io</li> <li>lightning.ai</li> <li>Hyperbolic.xyz</li> <li>Latitude AI</li> <li>Paperspace</li> <li>Jarvislabs</li> <li>HPC-AI</li> <li>GCP</li> <li>AWS</li> </ul>"},{"location":"gradio_hf/","title":"Hugging Face Guide","text":""},{"location":"gradio_hf/#new-model-setup","title":"New Model Setup","text":""},{"location":"gradio_hf/#uploading-a-model","title":"Uploading a Model","text":""},{"location":"gradio_hf/#create-a-model-repository","title":"Create a Model Repository","text":"<p>Go to Hugging Face and create a <code>New Model</code> under your profile, which essentially creates a repository for your models. Refer to Hugging Face Repositories Getting Started for detailed instructions. </p> <p>Once created, you can clone the repository and proceed with the next steps on your local machine.</p>"},{"location":"gradio_hf/#set-up-git-lfs-large-file-storage","title":"Set Up Git LFS (Large File Storage)","text":"<p>Hugging Face uses Git LFS to manage large files, such as model weights. Install and set up Git LFS as follows:</p> <pre><code>sudo apt install git-lfs\ngit lfs install\n</code></pre>"},{"location":"gradio_hf/#configure-hugging-face-interface","title":"Configure Hugging Face Interface","text":"<p>Install the Hugging Face Hub library and set up authentication:</p> <pre><code>pip install huggingface_hub\n\n# Configure Git to store credentials\ngit config --global credential.helper store\n\n# Log in to Hugging Face CLI\nhuggingface-cli login\n\n# Your token will be saved in the following location:\ncat $HOME/.cache/huggingface/token\n\n# Enable LFS support for large files\nhuggingface-cli lfs-enable-largefiles\n</code></pre>"},{"location":"gradio_hf/#push-a-large-file","title":"Push a Large File","text":"<p>Once Git LFS is configured, you can push large files just like a normal Git commit:</p> <pre><code>git add your_model\ngit commit -m \"Add a large file using Git LFS\"\ngit push\n</code></pre> <p>For more details on uploading models, refer to the Hugging Face Model Uploading Guide.</p>"},{"location":"gradio_hf/#downloading-a-model","title":"Downloading a Model","text":"<p>To download a model from Hugging Face, you can use the following Python code:</p> <pre><code>from huggingface_hub import hf_hub_download\n\nmodel_path = hf_hub_download(repo_id=\"fkariminejadasl/bird\", filename=\"45_best.pth\")\n</code></pre> <p>For more downloading options, refer to the Hugging Face Models Downloading Guide.</p>"},{"location":"gradio_hf/#creating-a-new-space-on-hugging-face","title":"Creating a New Space on Hugging Face","text":"<p>To start, navigate to Hugging Face and create a <code>New Space</code> under your profile. This will generate a repository for your application. For detailed guidance, refer to the Gradio Spaces documentation.</p> <p>Once your space is created, you can clone the repository to your local machine, add your <code>app.py</code> and <code>requirements.txt</code> files, and push the changes back to the repository. The application will launch automatically.</p>"},{"location":"gradio_hf/#application-code-and-dependencies","title":"Application Code and Dependencies","text":"<ul> <li> <p><code>app.py</code>: Your Gradio application code must be saved in a file named <code>app.py</code>.</p> </li> <li> <p><code>requirements.txt</code>: This file should list all the Python dependencies your app requires, including any custom libraries. An example <code>requirements.txt</code> might look like this:</p> </li> </ul> <pre><code>torch\ngradio\nhuggingface_hub\ngit+https://github.com/username/repository_name.git@branch_name\n</code></pre> <ul> <li>Note: The <code>branch_name</code> can also be replaced by <code>tag_name</code> or <code>commit_hash</code> depending on your need.</li> </ul> <p>If the <code>pyproject.toml</code> file for a custom library is located in a subdirectory within the repository, you can specify the subdirectory like this:</p> <pre><code>git+https://github.com/username/repository_name.git@branch_name#subdirectory=subdirectory_name\n</code></pre> <p>For further details on how to use <code>pip install</code> with various options, refer to the Pip Install Guide.</p>"},{"location":"gradio_src/","title":"Gradio in SRC (SURF Research Cloud)","text":""},{"location":"gradio_src/#create-a-workspace","title":"Create a Workspace","text":"<p>In the SRC Dashboard, create an <code>Ubuntu 2204 - SUDO enabled</code> machine. </p>"},{"location":"gradio_src/#install-miniconda","title":"Install Miniconda","text":"<p>Follow the instructions here or below:</p> <pre><code>mkdir /scratch/venv\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /scratch/venv/miniconda.sh\nbash /scratch/venv/miniconda.sh -b -u -p /scratch/venv/\nrm /scratch/venv/miniconda.sh\n/scratch/venv/bin/conda init bash\n</code></pre> <p>Open a new terminal.</p>"},{"location":"gradio_src/#setup-virtual-environment","title":"Setup Virtual Environment","text":"<pre><code>conda create -n p310 python=3.10\nconda activate p310\n</code></pre> <p>Add the <code>conda activate p310</code> command to <code>~/.bashrc</code>.</p>"},{"location":"gradio_src/#install-bird-classification-app","title":"Install Bird Classification App","text":"<p>In <code>/scratch</code>:</p> <pre><code>git clone https://github.com/fkariminejadasl/bird-behavior.git\npip install .[app]\n</code></pre>"},{"location":"gradio_src/#setup-nginx","title":"Setup Nginx","text":"<pre><code>sudo apt install nginx\n</code></pre> <p>Create a <code>testgradio</code> file with the content below by using <code>sudo vi /etc/nginx/sites-available/testgradio</code>. For more detailed information, refer to Running Gradio on Your Web Server with Nginx.</p> <p>Find the domain name in <code>/etc/hosts</code>.  </p> <pre><code>server {\n    listen 80;\n    server_name your_domain_name;  # This is in /etc/hosts\n\n    location / {  # Change this if you'd like to serve your Gradio app on a different path\n        proxy_pass http://127.0.0.1:7860/; # Change this if your Gradio app will be running on a different port\n        proxy_buffering off;\n        proxy_redirect off;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection \"upgrade\";\n        proxy_set_header Host $host;\n        proxy_set_header X-Forwarded-Host $host;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}\n</code></pre> <p>Since <code>include /etc/nginx/sites-enabled/*;</code> is added in the <code>/etc/nginx/nginx.conf</code> file, we need to create a symbolic link:</p> <pre><code>sudo ln -s /etc/nginx/sites-available/testgradio /etc/nginx/sites-enabled\n</code></pre>"},{"location":"gradio_src/#setup-gradio-app-as-a-service","title":"Setup Gradio App as a Service","text":"<p>Create the service file using <code>sudo vi /etc/systemd/system/gradio.service</code>:</p> <pre><code>[Unit]\nDescription=Service to launch Gradio app\nAfter=network.target\n\n[Service]\nUser=fkariminej\nGroup=www-data\nEnvironment=\"CONDA_BASE=/scratch/venv\"\nEnvironment=\"CONDA_ENV_NAME=p310\"\nWorkingDirectory=/scratch/bird-behavior/app\nExecStart=/bin/bash -c \"source $CONDA_BASE/bin/activate $CONDA_ENV_NAME &amp;&amp; python testgradio.py\"\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"},{"location":"gradio_src/#enable-services","title":"Enable Services","text":"<p>Handy commands in <code>systemctl</code>: status, start, enable, disable, stop.</p> <pre><code>sudo systemctl status nginx\nsudo systemctl status gradio\n</code></pre>"},{"location":"gradio_src/#acknowledgement","title":"Acknowledgement","text":"<p>This document is modified from the original document by Berend Wijers and loosely follows the Serve Python App on Nginx.</p>"},{"location":"improve_training_results/","title":"Improving Deep Learning Training Results","text":"<p>Deep learning training outcomes can significantly improve by focusing on three main components: data, model, and training. This framework can be expanded to \"data, model, loss, optimizer,\" as detailed in Andrej Karpathy's insightful blog post. He suggests dividing the model into model and loss, with training encapsulated by the optimizer. Here are strategies to enhance each component:</p>"},{"location":"improve_training_results/#data","title":"Data","text":"<ul> <li>Data Augmentation: Apply data augmentation techniques to combat overfitting.</li> <li>Expand Dataset: Increasing the dataset size can lead to more robust training outcomes.</li> </ul>"},{"location":"improve_training_results/#model","title":"Model","text":"<ul> <li>Dropout: Integrate dropout layers to prevent overfitting.</li> <li>Activation Functions: Utilize ReLU/GLUE for non-linear transformations.</li> <li>Loss Functions: Proper selection of loss functions is crucial.</li> </ul>"},{"location":"improve_training_results/#training-optimizerscheduler","title":"Training (Optimizer/Scheduler)","text":"<ul> <li>Epochs: Increasing the number of iterations or epochs, especially when using dropout, is often necessary for optimal performance.</li> <li>Learning Rate: Adjusting the learning rate is a primary method for enhancing performance.</li> <li>Optimizer Choice: Switching optimizers, e.g., from Adam to AdamW, can improve results due to AdamW's weight decay feature, which acts as a regularizer.</li> <li>Scheduling: Implementing schedulers, with or without options for warm-up phases, can fine-tune the learning rate adjustment process over time. Example schedulers include <code>StepLR</code>, <code>CosineAnnealingLR</code>, and <code>ExponentialLR</code>, with warm-up variations like <code>CosLR</code> and <code>LinearLR</code>. </li> </ul> <p>Note: Weight decay, Dropout and Label Smoothing are the basic regularizers. </p> <p>Note: The strategies outlined above are often detailed in academic papers.</p>"},{"location":"jupyter_src/","title":"Shared Jupyter Notebook in SRC (SURF Research Cloud)","text":"<p>In order to make a notebook available to all users, <code>conda</code>, a <code>virtual environment</code>, and a <code>Jupyter kernel</code> should be installed in a shared location, such as <code>/opt</code>, where all users have access.</p>"},{"location":"jupyter_src/#create-a-workspace","title":"Create a Workspace","text":"<p>Create a \"Jupyter Notebook\" workspace from \"Create new workspace\" and attach an external storage. The storage is mounted under \"/data/storage_name\". You can transfer data for example by scp:</p> <pre><code>scp -r your_data username@ip_address:/data/storage_name\n</code></pre>"},{"location":"jupyter_src/#setup-conda-for-all-users","title":"Setup Conda for All Users","text":"<pre><code># Create a Directory for Conda in a System-Wide Location:\nsudo mkdir -p /opt/miniconda3\n\n# Download the Miniconda Installer:\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /tmp/miniconda.sh\n\n# Install Miniconda to the System-Wide Location:\nsudo bash /tmp/miniconda.sh -b -u -p /opt/miniconda3\n\n# Remove the Installer Script:\nrm /tmp/miniconda.sh\n\n# Set Permissions for All Users:\nsudo chmod -R 755 /opt/miniconda3\nsudo chown -R root:root /opt/miniconda3\n\n# Add Conda to the PATH for All Users:\necho 'export PATH=\"/opt/miniconda3/bin:$PATH\"' | sudo tee -a /etc/profile.d/conda.sh\n# /opt/miniconda3/etc/profile.d/conda.sh\n\n# Initialize Conda for All Users' Shells:\n/opt/miniconda3/bin/conda init --all\n\n# Verify the Installation:\nconda --version\n</code></pre>"},{"location":"jupyter_src/#make-kernel-for-all-users","title":"Make Kernel for All Users","text":"<pre><code># Activate Conda\nsource /opt/miniconda3/bin/activate\n\n# Run Conda as Root Using Full Path\nsudo /opt/miniconda3/bin/conda create -y -p /opt/miniconda3/envs/p310 python=3.10\n\n# Set Permissions\nsudo chmod -R 755 /opt/miniconda3/envs/p310\nsudo chown -R root:root /opt/miniconda3/envs/p310\n\n# Install ipykernel in the New Environment\nconda activate /opt/miniconda3/envs/p310\nsudo /opt/miniconda3/envs/p310/bin/pip install ipykernel\n\n# Register a System-Wide Jupyter Kernel: Create a new kernel specification that all users can access\n# This places the kernel spec in a system directory (e.g., /usr/local/share/jupyter/kernels/p310).\nsudo /opt/miniconda3/envs/p310/bin/python -m ipykernel install --name p310 --display-name \"bird\"\n\n# Permissions for Kernel Directory\nsudo chmod -R 755 /usr/local/share/jupyter/kernels/p310\n\n# Confirm the Installation: You should see p310 listed.\njupyter kernelspec list\n</code></pre>"},{"location":"jupyter_src/#install-your-package-for-all-users","title":"Install Your Package for All Users","text":"<pre><code># Navigate to your desired directory\ncd /scratch\n\n# Clone the GitHub repo\ngit clone https://github.com/fkariminejadasl/bird-behavior.git\ncd bird-behavior\n\n# Checkout the specific branch\ngit checkout cleanup1\n\n# Install the package into the p310 environment\nsudo /opt/miniconda3/envs/p310/bin/pip install -e .\n</code></pre>"},{"location":"linux/","title":"Linux","text":""},{"location":"linux/#shared-directories","title":"Shared Directories","text":"<p>Linux distributions typically follow the Filesystem Hierarchy Standard (FHS), which defines where different types of files are placed. Some common system-wide (shared) directories are:</p> <ul> <li>/etc: System-wide configuration files.  </li> <li>/usr: Contains the majority of userland programs and data.  </li> <li>/usr/bin: Executables for most user programs.  </li> <li>/usr/sbin: Executables for system administration.  </li> <li>/usr/lib: Shared libraries and internal binaries used by programs in <code>/usr/bin</code> and <code>/usr/sbin</code>.  </li> <li>/usr/local: For programs installed locally (outside of the distribution\u2019s package manager), typically in subdirectories like <code>/usr/local/bin</code> and <code>/usr/local/lib</code>.  </li> <li>/usr/share: Architecture-independent data such as icons, documentation, and locale files.</li> <li>/var: Variable data that changes during system operation.  </li> <li>/var/lib: State information and variable data for system programs.</li> <li>/var/log: Log files.</li> <li>/opt: Optional or third-party software, often self-contained packages.</li> <li>/lib, /lib64: Core system libraries (for <code>/bin</code> and <code>/sbin</code>).</li> </ul> <p>When you install packages using a package manager like <code>apt</code>, the files typically go into these shared directories depending on their type:</p> <ul> <li>Executables usually end up in <code>/usr/bin</code> or <code>/usr/sbin</code>.</li> <li>Libraries go into <code>/usr/lib</code>.</li> <li>Configuration files go into <code>/etc</code>.</li> <li>Documentation and data files often end up in <code>/usr/share</code>.</li> </ul> <p>These directories are accessible system-wide, so all users on the system can use the installed programs without additional per-user setup.</p>"},{"location":"linux/#cheat-sheet","title":"Cheat Sheet","text":""},{"location":"linux/#file-and-directory-management","title":"File and Directory Management","text":"Command Description <code>ls</code> List files and directories in the current directory. <code>ls -l</code> List files with detailed information. <code>ls -a</code> List all files, including hidden ones. <code>cd [directory]</code> Change the current directory to the specified one. <code>pwd</code> Display the current working directory path. <code>mkdir [directory]</code> Create a new directory. <code>rmdir [directory]</code> Remove an empty directory. <code>rm [file]</code> Delete a file. <code>rm -r [directory]</code> Recursively delete a directory and its contents. <code>cp [source] [destination]</code> Copy files or directories. <code>mv [source] [destination]</code> Move or rename files or directories. <code>touch [file]</code> Create an empty file or update the timestamp of an existing file. <code>cat [file]</code> Display the contents of a file. <code>less [file]</code> View the contents of a file one screen at a time. <code>head [file]</code> Display the first 10 lines of a file. <code>tail [file]</code> Display the last 10 lines of a file. <code>diff [file1] [file2]</code> Compare two files line by line and display the differences. <code>readlink [link]</code> Display the target of a symbolic link. <code>readlink -f [path]</code> Display the absolute path, resolving all symbolic links. <code>ln [target] [link_name]</code> Create a hard link to a file. <code>ln -s [target] [link_name]</code> Create a symbolic (soft) link to a file or directory."},{"location":"linux/#file-permissions-and-ownership","title":"File Permissions and Ownership","text":"Command Description <code>chmod [permissions] [file]</code> Change the permissions of a file or directory. <code>chown [owner]:[group] [file]</code> Change the owner and group of a file or directory. <code>chgrp [group] [file]</code> Change the group of a file or directory."},{"location":"linux/#process-management","title":"Process Management","text":"Command Description <code>ps</code> Display information about active processes. <code>pgrep: ps + grep</code> Search for processes by name or other attributes and display their PIDs. <code>top</code> Display real-time system information, including active processes. <code>htop</code> Interactive process viewer (requires installation). <code>nvtop</code> Interactive monitor for NVIDIA GPUs (requires NVIDIA GPUs). <code>kill [PID]</code> Terminate a process by its Process ID (PID). <code>killall [process_name]</code> Terminate all processes with the specified name. <code>bg</code> Resume a suspended job in the background. <code>fg</code> Bring a background job to the foreground."},{"location":"linux/#disk-usage-and-storage","title":"Disk Usage and Storage","text":"Command Description <code>df -h</code> Display disk space usage in human-readable format. <code>du -sh [directory]</code> Display the size of a directory and its contents. <code>mount [device] [mount_point]</code> Mount a device to the filesystem. <code>umount [device]</code> Unmount a device from the filesystem."},{"location":"linux/#networking","title":"Networking","text":"Command Description <code>ifconfig</code> Display or configure network interfaces. <code>ip a</code> Display all network interfaces and their IP addresses. <code>ping [host]</code> Send ICMP ECHO_REQUEST packets to network hosts. <code>wget [url]</code> Download files from the internet. <code>curl [url]</code> Transfer data from or to a server. <code>ssh [user]@[host]</code> Connect to a remote host via SSH."},{"location":"linux/#file-transfer","title":"File Transfer","text":"Command Description <code>scp [source] [user@host:destination]</code> Securely copy files between hosts over a network. <code>rsync [options] [source] [destination]</code> Synchronize files and directories between two locations efficiently."},{"location":"linux/#user-management","title":"User Management","text":"Command Description <code>adduser [username]</code> Create a new user. <code>passwd [username]</code> Change the password for a user. <code>deluser [username]</code> Delete a user. <code>usermod -aG [group] [username]</code> Add a user to a group."},{"location":"linux/#system-information","title":"System Information","text":"Command Description <code>uname -a</code> Display all system information. <code>uname -r</code> Display the kernel version. <code>uptime</code> Show how long the system has been running. <code>date</code> Display or set the system date and time. <code>who</code> Show who is logged into the system. <code>whoami</code> Display the current logged-in user's username."},{"location":"linux/#package-management-debian-based-systems","title":"Package Management (Debian-based systems)","text":"Command Description <code>apt update</code> Update the package index. <code>apt upgrade</code> Upgrade all installed packages to their latest versions. <code>apt install [package]</code> Install a new package. <code>apt remove [package]</code> Remove an installed package. <code>apt search [package]</code> Search for a package in the repositories."},{"location":"linux/#text-processing","title":"Text Processing","text":"Command Description <code>grep [pattern] [file]</code> Search for a pattern in a file. <code>sed 's/[old]/[new]/' [file]</code> Replace text in a file using stream editor. <code>awk '{print $1}' [file]</code> Pattern scanning and processing language."},{"location":"linux/#compression-and-archiving","title":"Compression and Archiving","text":"Command Description <code>tar -cvf [archive.tar] [files]</code> Create a tarball archive of files. <code>tar -xvf [archive.tar]</code> Extract files from a tarball archive. <code>gzip [file]</code> Compress a file using gzip. <code>gunzip [file.gz]</code> Decompress a gzip compressed file. <p>This cheat sheet provides a quick reference to common Linux commands. For more detailed information, refer to the manual pages by typing <code>command --help</code> or <code>man command</code> in the terminal. </p>"},{"location":"linux/#examples","title":"Examples","text":"<pre><code># Copy a local file to a remote host:\nscp /path/to/local/file.txt user@remote_host:/path/to/remote/directory/\n\n# Copy a file from a remote host to the local machine:\nscp user@remote_host:/path/to/remote/file.txt /path/to/local/directory/\n\n# Synchronize a local directory to a remote host:\n# `-a`: Archive mode (preserves permissions, times, symbolic links, etc.).\n# `-v`: Verbose output.\n# `-z`: Compress data during transfer.\nrsync -avz /path/to/local/directory/ user@remote_host:/path/to/remote/directory/\n\n# Synchronize a remote directory to the local machine:\nrsync -avz user@remote_host:/path/to/remote/directory/ /path/to/local/directory/\n\n# Find the process ID(s) of a running program:\npgrep process_name\n\n# Find processes by user:\npgrep -u username\n\n# Find differences\ndiff --color -U 0 file1 file2\n\n# Display the absolute path\nreadlink -f $HOME\n\n# To create a symbolic link named `my_link` that points to a file `myfile.txt`\nln -s myfile.txt my_link\n\n# To unzip a file to a specific location\nunzip filename.zip -d /path/to/destination\n</code></pre>"},{"location":"object_detection/","title":"Object Detection Manual: A Quick Overview","text":""},{"location":"object_detection/#introduction","title":"Introduction","text":"<p>Object detection is a critical task in computer vision, involving the classification and localization of objects within an image or video. This manual provides a quick overview of various methods to enhance the efficiency and accuracy of object detection. We'll explore different categories of object detection, including Faster R-CNN, YOLO (You Only Look Once), CenterNet, and DETR (DEtection Transformer). Finally, we'll delve into open-set object detection and object detection with limited data.</p>"},{"location":"object_detection/#categories-of-object-detection-methods","title":"Categories of Object Detection Methods","text":"<p>Object detection methods can be categorized based on key attributes influencing their design and performance. Here are prominent categories:</p> <ul> <li>Two-Stage vs. One-Stage Methods:</li> <li>Two-stage methods, like Faster R-CNN, involve region proposal and subsequent classification, offering high accuracy but may be slower.</li> <li> <p>One-stage methods, such as YOLO, perform object detection in a single step, providing faster inference for real-time applications.</p> </li> <li> <p>Anchor-Based vs. Anchor-Free Methods:</p> </li> <li> <p>Anchor-based methods, like Faster R-CNN, use predefined anchor boxes, while anchor-free methods, such as CenterNet, eliminate the need for predefined anchors.</p> </li> <li> <p>Region-Based vs. Query-Based Methods:</p> </li> <li> <p>Region-based methods divide an image into regions (e.g., Faster R-CNN), while query-based methods like DETR use transformer architectures for set prediction.</p> </li> <li> <p>Plain vs. Hierarchical Methods:</p> </li> <li>Plain methods maintain a single-scale feature map, e.g., ViTDet, while hierarchical methods contain multi-scale features.</li> </ul>"},{"location":"object_detection/#two-stage-methods-faster-r-cnn","title":"Two-Stage Methods: Faster R-CNN","text":"<p>Faster R-CNN (Region-based Convolutional Neural Network):</p> <ul> <li> <p>Overview: A two-stage framework combining region proposal and object classification using a region proposal network (RPN).</p> </li> <li> <p>Links: R-CNN, Fast R-CNN, Faster R-CNN</p> </li> </ul>"},{"location":"object_detection/#one-stage-methods-yolo","title":"One-Stage Methods: YOLO","text":"<p>YOLO (You Only Look Once):</p> <ul> <li> <p>Overview: A one-stage algorithm dividing the image into a grid and predicting bounding boxes and class probabilities directly for real-time object detection.</p> </li> <li> <p>Links: YOLO, YOLO brief history</p> </li> </ul>"},{"location":"object_detection/#anchorless-methods-centernet","title":"Anchorless Methods: CenterNet","text":"<p>CenterNet:</p> <ul> <li> <p>Overview: An anchorless approach focusing on predicting object centers and regressing bounding box coordinates directly, eliminating the need for predefined anchors.</p> </li> <li> <p>Links: CenterNet</p> </li> </ul>"},{"location":"object_detection/#transformer-based-methods-detr","title":"Transformer-Based Methods: DETR","text":"<p>DETR (DEtection Transformer):</p> <ul> <li> <p>Overview: A transformer-based object detection model formulating object detection as a set prediction problem, simultaneously predicting object classes and bounding box coordinates.</p> </li> <li> <p>Links: DETR, deformableDETR</p> </li> </ul>"},{"location":"object_detection/#open-set-object-detection-open-vocabulary-object-detection-ovd","title":"Open-Set Object Detection | Open Vocabulary Object Detection (OVD)","text":"<ul> <li> <p>Overview: Open-set object detection, or open vocabulary object detection, aims to detect objects of novel categories beyond the training vocabulary. Traditional models are limited to a fixed set, but open-set detection scales up the vocabulary size.</p> </li> <li> <p>Links: Grounding DINO, OWL-VIT, Detic, paperwithcode list.</p> </li> </ul>"},{"location":"object_detection/#object-detection-with-limited-data","title":"Object Detection with Limited Data","text":"<p>To address the challenge of limited labeled data, leveraging pre-training in self-supervised learning is an effective strategy. Two prominent methods are contrastive learning and reconstruction-based methods. In contrastive learning, data augmentation is applied, and the model learns by bringing the representation of augmented parts together while pushing non-augmented parts further apart. Another method involves removing part of the data, and the model attempts to reconstruct the missing portion, as seen in Masked Autoencoders (MAE).</p> <p>Alternatively, foundation models\u2014large models trained on extensive datasets\u2014can serve as a pre-training step. These pre-trained models can be fine-tuned on specific tasks using a smaller dataset or used to distill knowledge into a smaller model, minimizing size while preserving performance.</p> <p>Another common approach involves training with different modalities, particularly text and image data, in a self-supervised manner. Following the success of models like CLIP, various methods, such as Grounding DINO and OWL-VIT, have adopted this approach for training.</p> <p>Disclaimer: Part of this manual was generated by ChatGPT and you.com with my modification.</p>"},{"location":"postgres_plpython3u/","title":"Set Up a Custom Python Function within PostgreSQL","text":"<p>Before proceeding with the following steps, ensure you have set up a new environment on your system with a custom Python virtual environment located at <code>/scratch/venv/envs/p310</code>. For additional setup instructions, refer to apps.md.</p> <p>Follow these steps to get everything working from scratch:</p>"},{"location":"postgres_plpython3u/#step-1-install-postgresql-and-plpython3u","title":"Step 1: Install PostgreSQL and <code>plpython3u</code>","text":""},{"location":"postgres_plpython3u/#install-postgresql-and-the-plpython-extension-plpython3u","title":"Install PostgreSQL and the PL/Python Extension (<code>plpython3u</code>):","text":"<p>Run the following commands to install PostgreSQL and the <code>plpython3u</code> extension:</p> <pre><code>sudo apt update\nsudo apt install postgresql-plpython3-14\n</code></pre>"},{"location":"postgres_plpython3u/#enable-plpython3u-in-your-database","title":"Enable <code>plpython3u</code> in Your Database:","text":"<p>To enable <code>plpython3u</code> in a specific database (e.g., <code>test</code>), first log into PostgreSQL:</p> <pre><code>sudo -u postgres psql\n</code></pre> <p>Then, create your test database (if it doesn\u2019t already exist) and enable the extension:</p> <pre><code>CREATE DATABASE test; \n\\connect test\nCREATE EXTENSION plpython3u;\n</code></pre> <p>Exit the <code>psql</code> interface:</p> <pre><code>\\q\n</code></pre>"},{"location":"postgres_plpython3u/#step-2-create-a-postgresql-user-for-fkariminej","title":"Step 2: Create a PostgreSQL User for <code>fkariminej</code>","text":"<p>To facilitate authentication, create a PostgreSQL user matching your Linux system user.</p>"},{"location":"postgres_plpython3u/#create-a-postgresql-user","title":"Create a PostgreSQL User:","text":"<p>Open the PostgreSQL shell as the <code>postgres</code> superuser:</p> <pre><code>sudo -u postgres psql\n</code></pre> <p>Create the PostgreSQL user <code>fkariminej</code> with superuser privileges:</p> <pre><code>CREATE USER fkariminej WITH SUPERUSER CREATEDB CREATEROLE;\n</code></pre> <p>Exit the shell:</p> <pre><code>\\q\n</code></pre>"},{"location":"postgres_plpython3u/#access-the-database","title":"Access the Database:","text":"<p>From now on, you can directly access the database with the following command:</p> <pre><code>psql -U fkariminej -d test\n</code></pre>"},{"location":"postgres_plpython3u/#ensure-access-permissions","title":"Ensure Access Permissions:","text":"<p>Make sure the <code>postgres</code> user has access to your home directory and the relevant paths:</p> <pre><code>sudo chmod o+rx /home/fkariminej\nsudo chmod o+rx /scratch\nsudo chmod o+rx /scratch/venv\nsudo chmod o+rx /scratch/venv/envs\nsudo chmod o+rx /scratch/venv/envs/p310\n</code></pre>"},{"location":"postgres_plpython3u/#step-3-create-a-shell-script-to-start-postgresql-with-python-environment-variables","title":"Step 3: Create a Shell Script to Start PostgreSQL with Python Environment Variables","text":""},{"location":"postgres_plpython3u/#create-a-shell-script-to-start-postgresql","title":"Create a Shell Script to Start PostgreSQL:","text":"<p>Open a new shell script file for PostgreSQL:</p> <pre><code>vi /scratch/pg_service.sh\n</code></pre>"},{"location":"postgres_plpython3u/#add-environment-variables-for-python-paths-and-executable","title":"Add Environment Variables for Python Paths and Executable:","text":"<p>Add the following lines to set the <code>PYTHONPATH</code> and <code>PYTHONUSERBASE</code> for your virtual environment:</p> <pre><code>#!/bin/bash\n\nexport PGDATABASE=test\nexport PGUSER=fkariminej\nexport PGPORT=5432\nexport PATH=/usr/lib/postgresql/14/bin:$PATH\nexport PYTHONUSERBASE=/scratch/venv/envs/p310\nexport PYTHONPATH=/scratch/venv/envs/p310/lib/python310.zip:/scratch/venv/envs/p310/lib/python3.10:/scratch/venv/envs/p310/lib/python3.10/lib-dynload:/scratch/venv/envs/p310/lib/python3.10/site-packages\n\n# Start PostgreSQL using the correct configuration file\npg_ctl -D /etc/postgresql/14/main -l /var/lib/postgresql/14/main/postgres_logfile start\n</code></pre>"},{"location":"postgres_plpython3u/#make-the-script-executable","title":"Make the Script Executable:","text":"<pre><code>chmod +x /scratch/pg_service.sh\n</code></pre>"},{"location":"postgres_plpython3u/#create-and-set-permissions-for-the-logfile","title":"Create and Set Permissions for the Logfile:","text":"<pre><code>sudo touch /var/lib/postgresql/14/main/postgres_logfile\nsudo chown postgres:postgres /var/lib/postgresql/14/main/postgres_logfile\n</code></pre>"},{"location":"postgres_plpython3u/#stop-the-currently-running-postgresql-instance","title":"Stop the Currently Running PostgreSQL Instance:","text":"<pre><code>sudo systemctl stop postgresql@14-main.service\n</code></pre> <p>You can find the services:</p> <pre><code>systemctl list-units --type=service | grep postgresql\n</code></pre>"},{"location":"postgres_plpython3u/#run-the-script-as-the-postgres-user","title":"Run the Script as the <code>postgres</code> User:","text":"<pre><code>sudo -u postgres /scratch/pg_service.sh\n</code></pre>"},{"location":"postgres_plpython3u/#verify-the-configuration","title":"Verify the Configuration:","text":"<p>After running the script, verify the setup:</p> <pre><code>psql -U fkariminej -d test\n</code></pre>"},{"location":"postgres_plpython3u/#create-and-test-a-plpython-function","title":"Create and Test a PL/Python Function:","text":"<p>Create a PL/Python function and run it to ensure everything works:</p> <pre><code>CREATE OR REPLACE FUNCTION pymtorch()\nRETURNS float8 AS $$\nimport sys\nimport torch\nx = torch.Tensor([1, 3]) * torch.Tensor([4, 3]).to(torch.float32)\nreturn float(x.sum())\n$$ LANGUAGE plpython3u;\n\nSELECT pymtorch();\n</code></pre>"},{"location":"postgres_plpython3u/#step-4-create-a-new-systemd-service-for-automation","title":"Step 4: Create a New Systemd Service for Automation","text":"<p>To avoid manually stopping and restarting PostgreSQL every time the machine starts, create a new service:</p>"},{"location":"postgres_plpython3u/#create-a-new-systemd-service-file","title":"Create a New Systemd Service File:","text":"<pre><code>sudo vi /etc/systemd/system/newpostgres.service\n</code></pre>"},{"location":"postgres_plpython3u/#add-the-following-configuration","title":"Add the Following Configuration:","text":"<pre><code>[Unit]\nDescription=Custom PostgreSQL Service using /scratch/pg_service.sh\nAfter=network.target\n\n[Service]\nType=oneshot\nRemainAfterExit=yes\nExecStart=/bin/bash -c \"sudo systemctl stop postgresql@14-main.service &amp;&amp; sudo -u postgres /scratch/pg_service.sh\"\nExecStop=/bin/bash -c \"sudo -u postgres /usr/lib/postgresql/14/bin/pg_ctl -D /etc/postgresql/14/main stop\"\nUser=fkariminej\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"},{"location":"postgres_plpython3u/#reload-systemd-and-enable-the-new-service","title":"Reload Systemd and Enable the New Service:","text":"<pre><code>sudo systemctl daemon-reload\nsudo systemctl enable newpostgres.service\nsudo systemctl start newpostgres.service\nsudo systemctl status newpostgres.service\n</code></pre>"},{"location":"postgres_plpython3u/#clean-up-residual-files-if-needed","title":"Clean Up Residual Files if Needed:","text":"<p>If issues arise, clean up any leftover files:</p> <pre><code>sudo rm -f /var/lib/postgresql/14/main/postmaster.pid\nsudo rm -f /var/run/postgresql/.s.PGSQL.5432\n</code></pre>"},{"location":"postgres_plpython3u/#check-for-active-connections","title":"Check for Active Connections:","text":"<pre><code>sudo lsof -i :5432\n</code></pre>"},{"location":"postgres_plpython3u/#verify-postgresql-processes-are-not-running","title":"Verify PostgreSQL Processes Are Not Running:","text":"<pre><code>ps aux | grep postgres\nsudo kill -9 &lt;PID&gt;\n</code></pre>"},{"location":"postgres_plpython3u/#re-enable-the-new-service","title":"Re-enable the New Service:","text":"<pre><code>sudo systemctl enable newpostgres.service\n</code></pre>"},{"location":"postgres_plpython3u/#notes","title":"Notes","text":"<p>The <code>postgresql@14-main.service</code> is located in <code>/lib/systemd/system/{postgresql@.service, postgresql.service}</code>. Their symbolic links are found here:</p> <pre><code>'/etc/systemd/system/multi-user.target.wants/postgresql@14-main.service' -&gt; '/lib/systemd/system/postgresql@.service'\n/etc/systemd/system/multi-user.target.wants/postgresql.service -&gt; /lib/systemd/system/postgresql.service\n</code></pre>"},{"location":"postgres_plpython3u/#other-important-paths","title":"Other important paths:","text":"<pre><code>/usr/lib/postgresql/14/bin\n/var/lib/postgresql/14/main  # owner:group -&gt; postgres:postgres\n/etc/postgresql/14/main      # pg_ctl.conf -&gt; owner:group postgres:postgres\n</code></pre>"},{"location":"postgres_plpython3u/#sql-commands-cheat-sheet","title":"SQL Commands Cheat Sheet","text":"<pre><code>/*\nmultiple comments\n*/\nCREATE DATABASE test; \nDROP DATABASE test; \n\\connect test \nCREATE EXTENSION plpython3u; \nCREATE USER fkariminej WITH SUPERUSER CREATEDB CREATEROLE;\nSHOW data_directory; -- PGDATA SHOW data_directory;\nSELECT current_user; -- PGUSER here fkariminej\nSELECT current_database(); -- PGDATABASE here test\n\n-- A specific function (e.g. my_func), a specific schema (e.g., my_schema)\nSELECT my_func(); -- Run the function\nDROP FUNCTION my_func; -- Remove a function\nDROP FUNCTION my_func(integer, text); -- Remove a function with parameter\nDROP FUNCTION my_schema.my_func; -- Remove a function exists in a specific schema\nDROP FUNCTION IF EXISTS my_func;\n\n-- Print the definition of a function\nSELECT pg_get_functiondef(oid) \nFROM pg_proc \nWHERE proname = 'my_func';\n\n-- Print the definition of a function with Schema Context\nSELECT pg_get_functiondef(p.oid) \nFROM pg_proc p\nJOIN pg_namespace n ON p.pronamespace = n.oid\nWHERE p.proname = 'my_func' AND n.nspname = 'my_schema';\n\n-- Output the function definition in a file\n\\o output_file.sql\nSELECT pg_get_functiondef(oid) \nFROM pg_proc \nWHERE proname = 'my_func';\n\\o\n\n\\q -- Exit\n</code></pre> <pre><code>sudo -u postgres psql\nsudo -u postgres /scratch/pg_service.sh\npsql -U fkariminej -d test\n</code></pre>"},{"location":"practical_info_data/","title":"Some Practical Information on Data and Training","text":""},{"location":"practical_info_data/#data-transfer","title":"Data Transfer","text":""},{"location":"practical_info_data/#copy-data-from-surfdrive-research-drive-to-remote-machine-google-colabsnellius","title":"Copy Data from Surfdrive / Research Drive to Remote Machine (Google Colab/Snellius)","text":"<p>You can copy your data to Surfdrive, UvA Research Drive, or SURF Research Drive by zipping your data and using drag and drop to transfer the data. Then, use the share icon to share your data with a password and set it to have no expiration time. Use the \"Copy to Clipboard\" option to obtain the link. You will receive a link similar to <code>https://surfdrive.surf.nl/files/index.php/s/IS00bWerWu3MDJS</code>. The last element, e.g., <code>IS00bWerWu3MDJS</code>, represents your username. Utilize this username along with the password you specified when sharing this folder, as shown in the signature below, to download your data.</p> <pre><code>curl -u username:password surf_public_link -o your_output_file\n</code></pre> <p>Public link - The Surfdrive: https://surfdrive.surf.nl/files/public.php/webdav - The UvA Research Drive: https://uva.data.surfsara.nl/public.php/webdav - The SURF Research Drive: https://researchdrive.surfsara.nl/public.php/webdav</p> <p>For the example provided above, here is the code to download the data using curl and then unzip the data. The entire process of obtaining the data and unzipping it took less than 2 minutes for 2.2GB of data. When using with the Google Colab, remember to prefix each command with the ! sign.</p> <pre><code>curl -u \"IS00bWerWu3MDJS:password\" \"https://surfdrive.surf.nl/files/public.php/webdav\" -o mot\nunzip mot -d mot_data &gt; /dev/null 2&gt;&amp;1\n</code></pre> <p>There is some information on SURF wiki for Research Drive, SURF wiki for SURFdrive or older one on surfnet for SURFdrive, but I found it unclear.</p>"},{"location":"practical_info_data/#copy-data-from-the-google-drive-to-the-google-colab","title":"Copy Data from the Google Drive to the Google Colab","text":"<p>The data can be dragged and dropped. Alternatively, you can copy your data using other methods:</p> <p>The below option is for sharing single file. It can be a zip file.</p> <p>Share the file with \"Share/Share/Anyone with the link\". Then \"Share/Copy Link\". You get the url like this: <code>https://drive.google.com/file/d/id_to_copy/view?usp=drive_link</code>. Use <code>id_to_copy</code> in <code>gdown</code>:</p> <pre><code>! pip install -U gdown requests\n! gdown id_to_copy --output /content/\n</code></pre> <p>The other option is to mount the whole Google drive (not recommanded):</p> <pre><code>from google.colab import drive\ndrive.mount(\"/content/drive\")\n!cp /content/test.yaml \"/content/drive/MyDrive\"\n</code></pre>"},{"location":"practical_info_data/#copy-data-from-crunchomics-to-snellius-and-reverse","title":"Copy Data from Crunchomics to Snellius and Reverse","text":"<p>N.B. You can only copy data via Crunchomics machine. Port 22 is closed on Snellius.</p> <pre><code># copy from Crunchomics to Snellius\nscp -r test.txt username@snellius.surf.nl:/home/username/test.txt\nrsync -avz --progress test.txt username@snellius.surf.nl:/home/username/test.txt\n\n# copy from Snellius to Crunchomics\nscp -r username@snellius.surf.nl:/home/username/test.txt .\nrsync -avz --progress username@snellius.surf.nl:/home/username/test.txt .\n</code></pre>"},{"location":"practical_info_data/#from-snellius-to-src-surf-research-cloud","title":"from Snellius to SRC (Surf Research Cloud)","text":"<p>In Snellius, generate ssh key and add it to SRC. Then create the environment in SRC. In Snellius, you can for example ssh, rsync and scp to this machine. If you have created a Jupyter notebook, the ip address can be found in the \"Workspace\" under \"Details\" of this Jupyter notebook. </p> <p>This option doesn't work for Crunchomics. Example:</p> <pre><code>ssh fkariminej@145.38.194.242\nscp fkariminej@145.38.194.242:/home/fkariminej/test.txt .\n</code></pre>"},{"location":"practical_info_data/#references","title":"References","text":"<ul> <li>Surfdrive</li> <li>SRC(SURF Research Cloud)</li> </ul>"},{"location":"python/","title":"Python Basics","text":""},{"location":"python/#setup-python","title":"Setup Python","text":"<p>Look at the guide in here or follow bellow steps.</p> <p>Install conda:</p> <pre><code>mkdir -p ~/miniconda3\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\nbash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\nrm ~/miniconda3/miniconda.sh\n</code></pre> <p>After installing, initialize your newly-installed Miniconda.</p> <pre><code>~/miniconda3/bin/conda init bash\n</code></pre> <p>Setup your python:</p> <pre><code>conda create -n some_name python=3.10\nconda activate some_name\n</code></pre> <p>You can put <code>conda activate some_name</code> in your <code>~/.bashrc</code> to activate it when open a new terminal.</p> <p>Remove your conda environment:</p> <pre><code>conda remove -n some_name --all\n</code></pre>"},{"location":"python/#remove-conda","title":"Remove Conda","text":"<p>To completely remove Miniconda from your Ubuntu system, including all associated configurations and files, follow these steps:</p>"},{"location":"python/#jupyter-kernel","title":"Jupyter Kernel","text":"<pre><code># This process installs a new Jupyter kernel named test with the display name \"bird\".\npip install ipykernel\nsudo python -m ipykernel install --name test --display-name bird\n\n# Location of the Installed Kernel\njupyter kernelspec list\n# This command will display a list of all installed kernels along with their corresponding paths. \n#  test       /home/your_username/.local/share/jupyter/kernels/test\n\n# Removing the Installed Kernel\njupyter kernelspec uninstall test\n</code></pre>"},{"location":"python/#deactivate-any-active-conda-environments","title":"Deactivate Any Active Conda Environments","text":"<p>Ensure that no Conda environments are active by running:</p> <pre><code>conda deactivate\n</code></pre>"},{"location":"python/#remove-the-miniconda-installation-directory","title":"Remove the Miniconda Installation Directory","text":"<p>Delete the directory where Miniconda is installed. By default, this is <code>~/miniconda3</code>. Use the following command, adjusting the path if necessary:</p> <pre><code>rm -rf ~/miniconda3/\n</code></pre> <p>Note: Replace <code>~/miniconda3/</code> with the correct path if you installed Miniconda elsewhere.</p>"},{"location":"python/#remove-conda-related-hidden-files-and-directories","title":"Remove Conda-Related Hidden Files and Directories","text":"<p>Remove hidden files and directories in your home directory that Conda uses:</p> <pre><code>rm -rf ~/.conda ~/.condarc ~/.continuum\n</code></pre> <p>These directories store Conda environments and settings.</p>"},{"location":"python/#remove-conda-initialization-from-shell-configuration","title":"Remove Conda Initialization from Shell Configuration","text":"<p>Conda adds initialization code to your shell's configuration file (e.g., <code>.bashrc</code>). To remove these lines:</p> <ul> <li>Open the configuration file in a text editor:</li> </ul> <p><code>bash   nano ~/.bashrc</code></p> <ul> <li>Scroll to the section managed by 'conda init', which looks like:</li> </ul> <p><code># &gt;&gt;&gt; conda initialize &gt;&gt;&gt;   # !! Contents within this block are managed by 'conda init' !!   __conda_setup=\"$('/home/username/miniconda3/bin/conda' 'shell.bash' 'hook' 2&gt; /dev/null)\"   if [ $? -eq 0 ]; then       eval \"$__conda_setup\"   else       if [ -f \"/home/username/miniconda3/etc/profile.d/conda.sh\" ]; then           . \"/home/username/miniconda3/etc/profile.d/conda.sh\"       else           export PATH=\"/home/username/miniconda3/bin:$PATH\"       fi   fi   unset __conda_setup   # &lt;&lt;&lt; conda initialize &lt;&lt;&lt;</code></p> <ul> <li> <p>Delete this entire block.</p> </li> <li> <p>Save the file and exit the editor (in nano, press <code>CTRL+O</code> to save and <code>CTRL+X</code> to exit).</p> </li> <li> <p>Apply the changes by sourcing the file:</p> </li> </ul> <p><code>bash   source ~/.bashrc</code></p>"},{"location":"python/#remove-any-remaining-conda-related-cache","title":"Remove Any Remaining Conda-Related Cache","text":"<p>Check for and remove any remaining Conda-related cache files:</p> <pre><code>rm -rf ~/.cache/conda\n</code></pre> <p>## Python Courses</p> <p>From https://software-carpentry.org/lessons, below courses are offered.  - Programming with Python - Plotting and Programming in Python</p> <p>For more on software engineering side, you could also attend this course: - Intermediate Research Software Development with Python</p>"},{"location":"pytorch_lightning/","title":"Pytorch Lightning","text":"<p>Integrating Hydra with PyTorch Lightning can significantly enhance the flexibility and scalability of your machine learning projects. Hydra simplifies configuration management, allowing you to easily modify hyperparameters and settings without altering your codebase. PyTorch Lightning streamlines the training process by providing a structured framework for PyTorch code. Additionally, PyTorch Lightning facilitates advanced features like mixed precision training, Fully Sharded Data Parallel (FSDP), Distributed Data Parallel (DDP) across multiple nodes, and multi-GPU training, making it a powerful choice for scaling and optimizing your deep learning workflows.</p>"},{"location":"pytorch_lightning/#example-integrating-hydra-with-pytorch-lightning","title":"Example: Integrating Hydra with PyTorch Lightning","text":"<pre><code>import torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\nimport lightning as L\nfrom omegaconf import DictConfig\nimport hydra\nfrom lightning.pytorch.loggers import WandbLogger\n\ntorch.set_float32_matmul_precision('high')\n\nclass LitModel(L.LightningModule):\n    def __init__(self, input_dim, hidden_dim, output_dim, learning_rate):\n        super(LitModel, self).__init__()\n        self.save_hyperparameters()\n        self.layer_1 = nn.Linear(input_dim * input_dim, hidden_dim)\n        self.layer_2 = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.layer_1(x))\n        x = self.layer_2(x)\n        return x\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        self.log('val_loss', loss)\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n\n@hydra.main(version_base=None, config_path=\"configs\", config_name=\"config\")\ndef main(cfg: DictConfig):\n    # Data\n    dataset = MNIST('', train=True, download=True, transform=transforms.ToTensor())\n    mnist_train, mnist_val = random_split(dataset, [55000, 5000])\n    train_loader = DataLoader(mnist_train, batch_size=32, num_workers=15)\n    val_loader = DataLoader(mnist_val, batch_size=32, num_workers=15)\n\n    # Model\n    model = LitModel(\n        input_dim=cfg.model.input_dim,\n        hidden_dim=cfg.model.hidden_dim,\n        output_dim=cfg.model.output_dim,\n        learning_rate=cfg.model.learning_rate\n    )\n\n    # Initialize W&amp;B logger\n    wandb_logger = WandbLogger(project='my-awesome-project')\n\n    # Trainer\n    trainer = L.Trainer(\n        accelerator='gpu',\n        devices=cfg.trainer.gpus,\n        max_epochs=cfg.trainer.max_epochs,\n        precision='16-mixed',\n        logger=wandb_logger,\n    )\n\n    # Training\n    trainer.fit(model, train_loader, val_loader)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"pytorch_lightning/#create-a-configuration-file","title":"Create a Configuration File:","text":"<pre><code># config.yaml\nmodel:\n  input_dim: 28\n  hidden_dim: 64\n  output_dim: 10\n  learning_rate: 0.001\n\ntrainer:\n  max_epochs: 10\n  gpus: 1\n</code></pre>"},{"location":"pytorch_lightning/#install-the-required-libraries","title":"Install the Required Libraries:","text":"<pre><code>pip install lightning hydra-core wandb\n</code></pre>"},{"location":"pytorch_lightning/#run-the-training-script","title":"Run the Training Script:","text":"<pre><code>python train.py\n</code></pre> <p>To override specific parameters without modifying the config.yaml file, use command-line arguments:</p> <pre><code>python train.py model.learning_rate=0.01 trainer.max_epochs=20\n</code></pre>"},{"location":"pytorch_lightning/#benefits-of-using-hydra-with-pytorch-lightning","title":"Benefits of Using Hydra with PyTorch Lightning:","text":"<ul> <li>Flexible Configuration Management: Hydra allows you to maintain a clean separation between code and configuration, facilitating easy experimentation with different settings.</li> <li>Command-Line Overrides: Easily adjust parameters via command-line arguments, enabling rapid testing of various configurations.</li> <li>Scalability: PyTorch Lightning's structured approach, combined with Hydra's configuration management, supports scaling from simple experiments to complex training pipelines.</li> </ul>"},{"location":"resource_limitations/","title":"Training with Resource Limitations","text":"<p>Training large deep learning models is notably resource-intensive, often presenting challenges in both memory and computational demands. In contrast, smaller models, while less demanding, may lack descriptive power. A practical approach to training larger models involves starting with a pre-trained model and then fine-tuning a small subset of parameters, typically the head of the model. If your training time is limited to just a few hours, it's advisable to checkpoint the model and resume training from the last saved checkpoint. Below, we present a variety of techniques designed to either reduce memory usage or/and enhance computational efficiency.</p> <ul> <li>Training: Cut Cross-Entropy (CCE), gradient accumulation, gradient checkpointing and CPU offloading. Gradient accumulation involves accumulating gradients over multiple mini-batches before updating model parameters. It's useful when the available memory is insufficient for a desired batch size. Gradient checkpointing (also known as activation checkpointing)) reduces memory usage by not saving intermediate tensors required for the backward pass. These tensors are recomputed during the backward pass, which increases computation time. CPU offloading stores weights in CPU RAM rather than on the GPU when they are not in use.</li> <li>Fine-Tuning Tricks: Fine-tune only a small number of parameters (PEFT), e.g., LoRA/controlNet.</li> <li>Specific to Attention Blocks in Transformers: FlashAttention, Flash-decoding.</li> <li>Tricks for GPU: Half-precision, quantization, paged optimizers (GPU to CPU transfer used in QLoRA for optimizer states). Examples are: fp32 -&gt; fp16/bf16 -&gt; int8 -&gt; nf4 (normal float 4-bit).</li> </ul>"},{"location":"resource_limitations/#inference-with-resource-limitations","title":"Inference with Resource Limitations","text":"<ul> <li>Model Parameters: <ul> <li>Model distillation: Distill a large model as a teacher model to a student model using distillation loss.</li> <li>Quantization techniques: Weight clustering (aka low-bit parallelization) is a compression technique.</li> </ul> </li> </ul>"},{"location":"resource_limitations/#nb","title":"N.B.","text":"<ul> <li>Memory consists of parameters (weights), gradients, optimizer states, and activations (batch size x largest layer).</li> <li>QLoRA freezes and quantizes the main model and adds a low-rank adapter (LoRA).</li> </ul>"},{"location":"resource_limitations/#references","title":"References","text":"<ul> <li>Fine-tuning of 7B model parameters on T4 from DeepLearning AI by Ludwig, presented by Travis Addair (watch from here to here.</li> <li>train a 70b language model on two 24GB GPUs: an open source system, based on FSDP and QLoRA, that can train a 70b model on two 24GB GPUs. They also used Gradient checkpointing, CPU offloading, and Flash Attention 2.</li> <li>LoRA</li> <li>SURF course on Profiling</li> </ul>"},{"location":"resource_limitations/#example-code","title":"Example code","text":"<p>Using fp16 (float16) in PyTorch:</p> <p>The detail explanation is in Automatic Mixed Precision and Example mixed precision training in pytroch.</p> <pre><code>import torch\n\n# Initialize model, optimizer, and other components\nmodel = MyModel().cuda()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n\nscaler = torch.GradScaler()\n\nfor inputs, labels in data_loader:\n    inputs, labels = inputs.cuda(), labels.cuda()\n\n    optimizer.zero_grad()\n\n    # Casts operations to mixed precision\n    with torch.autocast(device_type=\"cuda\", dtype=torch.float16)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, labels)\n\n    # Scales the loss and calls backward()\n    scaler.scale(loss).backward()\n\n    # Unscales gradients and calls optimizer step\n    scaler.step(optimizer)\n    scaler.update()\n</code></pre> <p>Using bf16 (bfloat16) in PyTorch:</p> <p>It can be the same as float16, without using scaler, or follow the code below.</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Check if bf16 is supported\nif torch.cuda.is_bf16_supported():\n    dtype = torch.bfloat16\nelse:\n    raise RuntimeError(\"Bfloat16 not supported on this device\")\n\n# Initialize model, optimizer, and other components\nmodel = MyModel().to(dtype=dtype, device='cuda')\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n\nfor inputs, labels in data_loader:\n    inputs, labels = inputs.to(dtype=dtype, device='cuda'), labels.to(device='cuda')\n\n    optimizer.zero_grad()\n\n    outputs = model(inputs)\n    loss = loss_fn(outputs, labels)\n\n    loss.backward()\n    optimizer.step()\n</code></pre>"},{"location":"resource_limitations/#pytorch-profiling","title":"Pytorch Profiling","text":"<p>The PyTorch Profiler is a tool that allows developers to understand and optimize their PyTorch code by analyzing its performance. Here's an example of setting up and using the PyTorch Profiler:</p>"},{"location":"resource_limitations/#code-example-with-pytorch-profiler","title":"Code Example with PyTorch Profiler","text":"<p>For more details take look at Tensorboard Profiler Tutorial.</p>"},{"location":"resource_limitations/#code-example","title":"Code Example","text":"<p>Here is a step-by-step example of setting up and using the PyTorch Profiler. </p> <pre><code>import torch\nimport torchvision.models as models\nfrom torchvision.models import ResNet18_Weights\nfrom torch.profiler import profile, record_function, ProfilerActivity\n\n# Set up a model and input data\nmodel = models.resnet18(weights=ResNet18_Weights.DEFAULT)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Generate dummy input data\ninput_data = torch.randn(8, 3, 224, 224).to(device)  # Batch of 8 images\n\n# Define the profiling configuration\nwith profile(\n    activities=[\n        ProfilerActivity.CPU,  # Monitor CPU activity\n        ProfilerActivity.CUDA  # Monitor CUDA activity (if applicable)\n    ],\n    on_trace_ready=torch.profiler.tensorboard_trace_handler(\"./log\"),  # Save data for TensorBoard\n    record_shapes=True,  # Record tensor shapes\n    with_stack=True  # Capture stack traces\n) as prof:\n\n    # Use record_function for specific profiling scopes\n    with record_function(\"model_inference\"):\n        output = model(input_data)  # Run the model inference\n\n# Analyze the profiler output\nprint(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n\n# Visualize the profiler output using TensorBoard:\n# Run `tensorboard --logdir=./log` in the terminal\n</code></pre> <ul> <li><code>profile</code> Context Manager: This manages the profiling session and specifies which activities (CPU, CUDA) to profile.</li> <li><code>record_function</code>: Labels a specific code block for profiling, so you can see its performance separately.</li> <li><code>tensorboard_trace_handler</code>: Saves the profiling results in a format compatible with TensorBoard.</li> <li><code>key_averages()</code>: Aggregates and summarizes profiling results for analysis in the console.</li> </ul> <p>You can customize the profiler to include:</p> <ul> <li>Custom intervals: Use <code>schedule</code> to specify profiling start and stop.</li> <li>Memory profiling: Set <code>profile_memory=True</code> to track memory usage.</li> <li>Exporting results: Save results to file using <code>prof.export_chrome_trace(\"trace.json\")</code>.</li> </ul> <p>Notes: Use smaller models or batches for testing, as profiling large models can generate a lot of data.</p>"},{"location":"resource_limitations/#visualize-the-profiler-output","title":"Visualize the profiler output","text":"<p>After generating a trace, simply drag the <code>trace.json</code> generated in <code>log</code> file (example above) into Perfetto UI or in chrome browser by typing <code>chrome://tracing</code> to visualize your profile.</p> <p>The TensorBoard integration with the PyTorch profiler is now deprecated. But if you still want to use TensorBoard you should install <code>pip install torch_tb_profiler</code> and then use <code>tensorboard --logdir=./log</code></p>"},{"location":"resource_limitations/#references_1","title":"References","text":"<ul> <li>Performance Tuning Guide</li> <li>What Every User Should Know About Mixed Precision Training in PyTorch</li> </ul>"},{"location":"scheuler/","title":"Scheduler","text":"<p>Here is a sample example to demonstrate how the scheduler changes the learning rate. You may choose to use a different scheduler.</p> <pre><code>import torch\nimport torch.nn as nn\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm import tqdm\n\n# Define a dummy model\nmodel = nn.Linear(10, 1)\n\n# Define optimizer and scheduler\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-2)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2000, gamma=0.1)\n\n# TensorBoard writer\nwriter = SummaryWriter(\"result/tensorboard/run1\")\n\n# Dummy training loop\nnum_epochs = 2500\nsteps_per_epoch = 10\n\nfor epoch in tqdm(range(num_epochs)):\n    for step in range(steps_per_epoch):\n        # Dummy input and loss\n        inputs = torch.randn(32, 10)\n        targets = torch.randn(32, 1)\n        outputs = model(inputs)\n        loss = nn.MSELoss()(outputs, targets)\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Log learning rate to TensorBoard\n        current_lr = round(optimizer.param_groups[-1][\"lr\"], 6)\n        global_step = epoch * steps_per_epoch + step\n        writer.add_scalar(\"Learning Rate\", current_lr, global_step)\n\n    # Update the scheduler\n    scheduler.step()\n\n    lr_optim = round(optimizer.param_groups[-1][\"lr\"], 6)\n    lr_sched = scheduler.get_last_lr()[0]\n    writer.add_scalar(\"lr/optim\", lr_optim, epoch)\n    writer.add_scalar(\"lr/sched\", lr_sched, epoch)\n\n\n# Close the writer\nwriter.close()\n</code></pre>"},{"location":"services/","title":"Services","text":""},{"location":"services/#service-desk","title":"Service Desk","text":"Name URL FNWI Service Desk serviceportal.uva.nl SURF Service Desk servicedesk.surf.nl"},{"location":"services/#services-direct-links","title":"Services Direct Links","text":"Service Name URL SURF Filesender filesender.surf.nl SURF Drive surfdrive.surf.nl SURF Research Cloud sram.surf.nl SURF Research Drive researchdrive.surfsara.nl FNWI Faculty Research Drive uva.data.surfsara.nl"},{"location":"tasks/","title":"Example Tasks for Large Language, Multimodal, and Vision Models","text":"<p>This text provides a detailed overview of various tasks that different large models, such as Large Language Models (LLMs), Large Multimodal Models (LMMs), and Large Vision Models (LVMs), are capable of handling, including text generation, translation, image analysis, multimodal reasoning, and more, illustrating the diverse applications of these advanced AI systems.</p>"},{"location":"tasks/#llm-large-language-model","title":"LLM (Large Language Model):","text":"<ul> <li> <p>Examples: </p> <ul> <li>Text generation and writing: content creation, summarization, paraphrasing, scriptwriting, poetry, social media posts, email drafting, product descriptions, resume/cover letter writing, letter writing.</li> <li>Text comprehension and analysis: question answering, text analysis, critical review, legal document analysis, code understanding.</li> <li>Translation and language tasks: language translation, language learning assistance, text correction, dialect and style adaptation.</li> <li>Research and information retrieval: fact-checking, information gathering, historical context, current events analysis.</li> <li>Creative and artistic assistance: creative brainstorming, character creation, plot development, meme creation.</li> <li>Programming and technical tasks: code generation, debugging assistance, algorithm explanation, data analysis, simulation, and modeling.</li> <li>Decision-making support: pros and cons analysis, risk assessment, scenario planning, strategic planning.</li> <li>Education and tutoring: subject tutoring, test preparation, essay assistance, learning resources.</li> <li>Communication and interaction: chatbot support, role-playing, interview simulation.</li> <li>Entertainment: game dialogue creation, trivia and quizzes, interactive storytelling.</li> <li>Personal assistance: task management, goal setting, personal advice.</li> <li>Customization and personalization: persona development, tone adjustment, content filtering.</li> <li>Simulation and modeling: scenario simulation, market analysis, behavioral modeling.</li> <li>Explanation: explaining complex concepts.</li> <li>Ethics and philosophy: ethical analysis, debate simulation.</li> </ul> </li> <li> <p>Examples from Llama 3: general knowledge and instruction following, code, math, reasoning, tool use (search engine, Python interpreter, mathematical computational engine), multilingual capabilities, long context (code reasoning, summarization, question answering).</p> </li> </ul>"},{"location":"tasks/#lmm-large-multimodal-model-vlm-vision-language-model","title":"LMM (Large Multimodal Model) / VLM (Vision Language Model):","text":"<ul> <li> <p>Examples from Unified-IO 2 (images, text, audio, action, points, bounding boxes, keypoints, and camera poses): image editing, image generation, free-form VQA, depth and normal generation, visual-based audio generation, robotic manipulation, reference image generation, multiview image completion, visual parsing and segmentation, keypoint estimation, visual audio localization, future frame prediction.</p> </li> <li> <p>Examples from 4M-21: multimodal retrieval (e.g., given image retrieve caption, segment), multimodal generation (e.g., given image generate depth), out-of-the-box (zero-shot) tasks (e.g., normal, depth, semantic segmentation, instance segmentation, 3D human pose estimation, clustering). </p> </li> <li> <p>Examples from ImageBind (images, text, audio, depth, thermal, and IMU): cross-modal retrieval, embedding-space arithmetic, audio to image generation.</p> </li> <li> <p>Examples from Llama 3 on image: visual grounding (grounding information such as points, bounding boxes, and masks), MMMU or multimodal reasoning (understand images and solve college-level problems in multiple-choice and open-ended questions), VQA / ChartQA / TextVQA / DocVQA (image, chart, diagram, text in the image).</p> </li> <li>Examples from Llama 3 on video: QA: PerceptionTest (answer temporal reasoning questions focusing on skills like memory, abstraction, physics, semantics, and different types of reasoning such as descriptive, explanatory, predictive, counterfactual), temporal and causal reasoning, with a focus on open-ended question answering, compositional reasoning requiring spatiotemporal localization of relevant moments, recognition of visual concepts, and joint reasoning with subtitle-based dialogue, reasoning over long video clips to understand actions, spatial relations, temporal relations, and counting.</li> <li> <p>Examples from Llama 3 on speech: speech recognition, speech translation, spoken question answering.</p> </li> <li> <p>Examples: something (audio, text) to image generation, open-set object detection, image captioning, image description, visual question answering, visual commonsense reasoning, text-based image retrieval, image-based text generation, text-guided image manipulation, data visualization.</p> </li> </ul>"},{"location":"tasks/#lvm-large-vision-model","title":"LVM (Large Vision Model)","text":"<ul> <li>Examples from SAM2: promptable visual segmentation (prompts: clicks, boxes, or masks), promptable segmentation.</li> <li>Examples from SAM: (prompts: point, box, segment, text): zero-shot single point valid mask evaluation (segmenting an object from a single foreground point), zero-shot edge detection, zero-shot object proposals, zero-shot instance segmentation, zero-shot text-to-mask. </li> <li>Fine-tuning on specific tasks. <ul> <li>Image task examples: classification, object detection, semantic segmentation, instance segmentation, image generation, style transfer, super-resolution, image inpainting, face recognition and analysis, optical character recognition, scene understanding, anomaly detection, gesture and pose recognition, image retrieval, emotion recognition, visual enhancement.</li> <li>Multiple images/video task examples: action recognition, video object detection, multiple object tracking, visual object tracking, track anypoint, optical flow, 3D object reconstruction, SLAM/SfM.</li> </ul> </li> </ul>"},{"location":"tasks/#references","title":"References","text":"<ul> <li>LLM, LMM, VLM: Llama 3, 4M-21, Unified-IO 2, Florence</li> <li>LVM: DINOv2, SAM, SAM 2. CLIP, SigLIP, ImageBind</li> </ul>"},{"location":"tasks/#sota-and-popular-off-the-shelf-models","title":"SOTA and Popular Off-the-Shelf Models:","text":"<p>The following list highlights some of the current state-of-the-art (SOTA) and previously leading methods used in various domains such as tracking, depth estimation, optical flow, 3D reconstruction, segmentation, and language models. These methods are selected based on papers and research studies I have read, where they were commonly employed as off-the-shelf solutions.</p> <p>Please note that the field of machine learning and computer vision is rapidly evolving, and new models are frequently introduced. This list reflects a snapshot of current practices, but advancements in the field may lead to newer and potentially better-performing techniques over time.</p> <ul> <li>Vision Encoders: Vision-Only Model Backbones: DINOv2, MAE (Masked Autoencoders), ResNet. Vision-Text Model Backbones: AIMv2, SigLIP, CLIP</li> <li>Depth Map: Depth Pro, DepthCrafter (for video), MiDaS, Depth Anything v2, DPT, ZoeDepth</li> <li>Optical Flow: SEA RAFT, RAFT</li> <li>3D Reconstruction: COLMAP (Non-Deep Learning), ACE Zero (ACE0), noposplat (potentially for sparse reconstruction), DuSt3R</li> <li>Point Matching: SuperPoint combined with lightGLUE or SuperGLUE, MASt3R, TAP (CoTracker3, TAPIR, PIP), SAM2 (Segment and Tracking Anything: SAM combined with DeAOT)</li> <li>Tracking: SAM2 (Segment Anything Model 2) for image and video</li> <li>Object Detection: Florence-2, Grounding DINO / DINO-X, PaliGemma 2</li> <li>Segmentation: Grounding DINO combined with SAM, Florence-2</li> <li>Pose Estimation: OpenPose</li> <li>Image Captioning: xGen-MM (BLIP-3), CogVLM2, PaliGemma 2</li> <li>Visual Question Answering: Any of the VLMs such as Phi-3.5, PaliGemma 2</li> <li>Text-to-Video Generation Models / Generative Video Models: CogVideoX (Tsinghua University), Stable Video Diffusion (Stability AI), Pika Labs (Pika Labs), Movie Gen and Emu Video (Meta), Sora (OpenAI), Gen-3 Alpha (Runway AI), Veo (Google DeepMind), HunyuanVideo (Tencent)</li> <li>Text-to-Image Generation Models: FLUX1 (Black Forest Labs), Ideogram v2 (Ideogram), Midjourney v6 (Midjourney), Stable Diffusion 3.5 (Stablity AI), DALLE 3 (OpenAI), Firefly 3 (Adobe), Imagen 3, Flamingo (Google DeepMind), Aurora of Grok (xAI)</li> <li>Large Language Models (LLMs):  Open source: LLAMA-3 (Meta), Phi-3 (Microsoft), Gemma (Google), Qwen (Alibaba), OLMo 2 (Ai2). Proprietary: Claude3 (Anthropic), Gemini (Google DeepMind), Deepseek</li> <li>Speech-to-Text: Whisper (OpenAI), Wav2Vec (Meta)</li> <li>Control Video by Action: Genie 2 (Google DeepMind)</li> <li>LMM: Nova (Amazon)</li> </ul>"},{"location":"tasks/#finding-models","title":"Finding Models","text":""},{"location":"tasks/#models-in-general","title":"Models in General","text":"<ul> <li>Hugging Face</li> <li>Roboflow contains the list of top models.</li> </ul>"},{"location":"tasks/#lmm-large-multimodal-model-vlm-vision-language-model_1","title":"LMM (Large Multimodal Model) / VLM (Vision Language Model):","text":"<ul> <li>Find open-source models: The Open VLM leaderboard shows the scores of top VLMs, and you can see which models are open-source or proprietary.</li> <li>Vision-Arena and Video-Arena Leaderboards from Hugging Face shows the current top VLMs.</li> <li>Text-to-Video</li> <li>Blog post on VLM: Implement a Vision Language Model from Scratch; Vision Language Using and Finetuning; vision language explanation;</li> </ul>"},{"location":"tasks/#llm-large-language-models","title":"LLM (Large Language Models)","text":"<ul> <li>Find open-source models: LLM Arena LMArena formerly LMSYS shows the current top LLMs. You can see which models are open-source or proprietary.</li> </ul>"},{"location":"tasks/#text-to-speech-and-speech-to-text","title":"Text-to-Speech and Speech-to-Text","text":"<ul> <li>Speech Arena</li> </ul>"},{"location":"tasks/#api-providers","title":"API Providers","text":"<p>Artificial Analysis</p>"},{"location":"tmux/","title":"Basic Session Management","text":"<ul> <li>Create a New Session: <code>tmux new -s session_name</code>   Or within an active session: <code>:new-session -s session_name</code></li> </ul>"},{"location":"tmux/#window-page-management","title":"Window (Page) Management","text":"<ul> <li> <p>Create a New Window (Page): <code>Ctrl-b c</code></p> </li> <li> <p>Switch Between Windows (Pages): <code>Ctrl-b n</code>  \u2014 This moves to the next window. <code>Ctrl-b p</code>  \u2014 This moves to the previous window. <code>Ctrl-b w</code>  \u2014 This lists all windows and allows you to select one to switch to. <code>Ctrl-b &lt;window number&gt;</code>  \u2014 This directly switches to the window by its number.</p> </li> <li> <p>Rename a Window: <code>Ctrl-b ,</code> \u2014 This allows you to rename the current window.</p> </li> <li> <p>Close a Window (Page): <code>Ctrl-b &amp;</code> \u2014 This closes the current window.</p> </li> </ul>"},{"location":"tmux/#pane-management-dividing-a-window","title":"Pane Management (Dividing a Window)","text":"<ul> <li> <p>Split Window Horizontally: <code>Ctrl-b %</code> \u2014 This splits the current window into two panes side by side.</p> </li> <li> <p>Split Window Vertically: <code>Ctrl-b \"</code> \u2014 This splits the current window into two panes, one above the other.</p> </li> <li> <p>Switch Between Panes: <code>Ctrl-b o</code>  \u2014 This cycles through open panes in the current window. <code>Ctrl-b ;</code>  \u2014 This toggles between the last two active panes. <code>Ctrl-b \u2190, \u2192, \u2191, \u2193</code>  \u2014 This navigates between panes in the respective direction.</p> </li> <li> <p>Resize Panes: <code>Ctrl-b :resize-pane -D</code> \u2014 Resize pane downwards. <code>Ctrl-b :resize-pane -U</code> \u2014 Resize pane upwards. <code>Ctrl-b :resize-pane -L</code> \u2014 Resize pane to the left. <code>Ctrl-b :resize-pane -R</code> \u2014 Resize pane to the right.</p> </li> <li> <p>Close a Pane: <code>Ctrl-b x</code> \u2014 This closes the current pane.</p> </li> </ul>"},{"location":"tmux/#session-management","title":"Session Management","text":"<ul> <li> <p>Detach from Session: <code>Ctrl-b d</code> \u2014 This detaches you from the current session, leaving it running in the background.</p> </li> <li> <p>Reattach to a Session: <code>tmux attach -t session_name</code></p> </li> <li> <p>List Sessions: <code>tmux ls</code>  \u2014 This lists all sessions  <code>Ctrl-b s</code> \u2014 This lists all sessions, allowing you to switch between them. </p> </li> <li> <p>Kill a Session: <code>Ctrl-b :kill-session -t session_name</code></p> </li> </ul>"},{"location":"transformer_models_with_variable_sequence_lengths/","title":"Handling Variable Sequence Lengths in Transformer Models","text":"<p>This guide provides code examples for handling variable sequence lengths in Transformer models using positional encodings. It includes examples of both sinusoidal (fixed) and learned positional embeddings, along with considerations when training on sequences of varying lengths.</p>"},{"location":"transformer_models_with_variable_sequence_lengths/#example-1-transformer-model-with-sinusoidal-positional-encoding","title":"Example 1: Transformer Model with Sinusoidal Positional Encoding","text":"<p>The following code demonstrates a Transformer model using sinusoidal positional encoding, which can handle variable sequence lengths without modification. </p> <p>This example is adapted from this source.</p> <pre><code>import math\nimport torch\nfrom torch import nn, Tensor\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\n\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        position = torch.arange(max_len).unsqueeze(1)  # Shape: [max_len, 1]\n        div_term = torch.exp(torch.arange(0, d_model, 2) * \n                             (-math.log(10000.0) / d_model))  # Shape: [d_model/2]\n\n        pe = torch.zeros(max_len, d_model)  # Shape: [max_len, d_model]\n        pe[:, 0::2] = torch.sin(position * div_term)  # Apply sine to even indices\n        pe[:, 1::2] = torch.cos(position * div_term)  # Apply cosine to odd indices\n        self.register_buffer('pe', pe)\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\n        Args:\n            x: Tensor of shape [seq_len, batch_size, d_model]\n        \"\"\"\n        x = x + self.pe[:x.size(0), :]  # Add positional encoding\n        return self.dropout(x)\n\nclass TransformerModel(nn.Module):\n\n    def __init__(self, ntoken: int, d_model: int, nhead: int, \n                 d_hid: int, nlayers: int, dropout: float = 0.5):\n        super().__init__()\n        self.pos_encoder = PositionalEncoding(d_model, dropout)\n        encoder_layers = TransformerEncoderLayer(d_model, nhead, \n                                                 d_hid, dropout)\n        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n        self.encoder = nn.Embedding(ntoken, d_model)\n        self.d_model = d_model\n        self.decoder = nn.Linear(d_model, ntoken)\n\n        self.init_weights()\n\n    def init_weights(self) -&gt; None:\n        initrange = 0.1\n        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n        nn.init.zeros_(self.decoder.bias)\n        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n\n    def forward(self, src: Tensor, src_mask: Tensor) -&gt; Tensor:\n        \"\"\"\n        Args:\n            src: Tensor of shape [seq_len, batch_size]\n            src_mask: Tensor of shape [seq_len, seq_len]\n        \"\"\"\n        src = self.encoder(src) * math.sqrt(self.d_model)\n        src = self.pos_encoder(src)\n        output = self.transformer_encoder(src, src_mask)\n        output = self.decoder(output)\n        return output\n\ndef generate_square_subsequent_mask(sz: int) -&gt; Tensor:\n    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n\n# Initialize the model\nntokens = 1000  # Vocabulary size\nd_model = 512   # Embedding dimension\nnhead = 8       # Number of attention heads\nd_hid = 2048    # Feedforward network dimension\nnlayers = 6     # Number of Transformer layers\ndropout = 0.5   # Dropout rate\n\nmodel = TransformerModel(ntokens, d_model, nhead, d_hid, nlayers, dropout)\n\n# Prepare input data with variable sequence length\nseq_len = 10    # Sequence length can vary\nbatch_size = 2  # Batch size\nx = torch.randint(0, ntokens, (seq_len, batch_size))  # Random input\n\nsrc_mask = generate_square_subsequent_mask(seq_len)\n\n# Run the model\noutput = model(x, src_mask)\n</code></pre>"},{"location":"transformer_models_with_variable_sequence_lengths/#example-2-transformer-model-with-learned-positional-embeddings","title":"Example 2: Transformer Model with Learned Positional Embeddings","text":"<p>In this example, positional embeddings are learned parameters, allowing the model to potentially capture position-specific patterns.</p> <pre><code>import torch\nfrom torch import nn, Tensor\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\n\nclass LearnedPositionalEncoding(nn.Module):\n\n    def __init__(self, max_len: int, d_model: int):\n        super().__init__()\n        self.pos_embedding = nn.Embedding(max_len, d_model)\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\n        Args:\n            x: Tensor of shape [seq_len, batch_size, d_model]\n        \"\"\"\n        seq_len = x.size(0)\n        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device)\n        position_ids = position_ids.unsqueeze(1).expand(seq_len, x.size(1))\n        position_embeddings = self.pos_embedding(position_ids)\n        return x + position_embeddings\n</code></pre>"},{"location":"transformer_models_with_variable_sequence_lengths/#example-3-transformer-model-with-learned-positional-embeddings-initialized-with-sinusoidal-positional-encoding","title":"Example 3: Transformer Model with Learned Positional Embeddings Initialized with Sinusoidal Positional Encoding","text":"<p>In this example, positional embeddings are learned parameters initialized with Sinusoidal Positional Encoding.</p> <pre><code>class SinusoidalInitializedPositionalEncoding(nn.Module):\n    def __init__(self, max_len: int, d_model: int):\n        super().__init__()\n        # Create a learnable positional embedding parameter\n        self.pos_embedding = nn.Parameter(torch.zeros(max_len, d_model))\n\n        # Initialize with sinusoidal positional encoding\n        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)  # [max_len, 1]\n        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) *\n                             (-math.log(10000.0) / d_model))  # [d_model/2]\n\n        sinusoidal_embedding = torch.zeros(max_len, d_model)  # [max_len, d_model]\n        sinusoidal_embedding[:, 0::2] = torch.sin(position * div_term)  # Even indices\n        sinusoidal_embedding[:, 1::2] = torch.cos(position * div_term)  # Odd indices\n\n        # Assign the sinusoidal values to the parameter without tracking gradients\n        with torch.no_grad():\n            self.pos_embedding.copy_(sinusoidal_embedding)\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        \"\"\"\n        Args:\n            x: Tensor of shape [seq_len, batch_size, d_model]\n        \"\"\"\n        seq_len = x.size(0)\n        x = x + self.pos_embedding[:seq_len, :].unsqueeze(1)\n        return x\n</code></pre>"},{"location":"transformer_models_with_variable_sequence_lengths/#considerations-when-training-with-learned-positional-embeddings","title":"Considerations When Training with Learned Positional Embeddings","text":"<p>Training a model with learned positional embeddings (PE) where the majority of sequence lengths are significantly shorter than the specified <code>max_len</code> can indeed present challenges:</p> <ul> <li> <p>Underrepresentation of Longer Positions: If most training sequences are short, embeddings corresponding to higher positions (i.e., positions near <code>max_len</code>) receive minimal updates during training. This lack of exposure can lead to poor generalization for longer sequences during inference, as the model hasn't adequately learned representations for these positions.</p> </li> <li> <p>Inefficient Resource Utilization: Allocating parameters for positions up to <code>max_len</code> consumes memory and computational resources. If these positions are seldom used during training, this allocation becomes inefficient.</p> </li> </ul>"},{"location":"transformer_models_with_variable_sequence_lengths/#mitigation-strategies","title":"Mitigation Strategies:","text":"<ul> <li> <p>Dynamic Positional Embeddings: Instead of a fixed <code>max_len</code>, employ dynamic positional embeddings that adjust based on the actual sequence lengths encountered during training. This approach ensures that the model learns appropriate embeddings for the positions it processes.</p> </li> <li> <p>Curriculum Learning: Start training with shorter sequences and progressively introduce longer ones. This method helps the model gradually adapt to various sequence lengths, ensuring that embeddings for higher positions are adequately trained.</p> </li> <li> <p>Data Augmentation: Artificially increase the length of training sequences by padding or concatenating sequences. This technique exposes the model to a broader range of positions, aiding in the learning of embeddings across the entire range up to <code>max_len</code>.</p> </li> <li> <p>Regularization Techniques: Apply regularization methods to prevent overfitting to shorter sequences, encouraging the model to generalize better to longer sequences.</p> </li> </ul>"},{"location":"transformer_models_with_variable_sequence_lengths/#summary","title":"Summary","text":"<ul> <li> <p>Sinusoidal Positional Encoding: Handles variable sequence lengths naturally without the need for learned parameters tied to specific positions.</p> </li> <li> <p>Learned Positional Embeddings: Require careful consideration of sequence length distribution in the training data to ensure embeddings for all positions are adequately trained.</p> </li> <li> <p>Training Strategies: Adjust your data and training process, not the model code, to handle variable sequence lengths effectively.</p> </li> </ul>"}]}